{
    "generation": "    public ZipArchiveEntry getNextZipEntry() throws IOException {\n        boolean firstEntry = true; // Used to determine if we need to call readFirstLocalFileHeader\n        if (closed || hitCentralDirectory) {\n            return null;\n        }\n        if (current != null) {\n            // closeEntry() should consume the remainder of the current entry's data,\n            // possibly reading the data descriptor if present.\n            closeEntry();\n            firstEntry = false;\n        }\n\n        try {\n            // LFH_BUF should be at least 30 bytes long (size of LFH fixed part).\n            if (firstEntry) {\n                // Special handling for potential split archive markers.\n                // On the first entry, readFirstLocalFileHeader might read past\n                // a split marker if present, before reading the LFH.\n                readFirstLocalFileHeader(LFH_BUF);\n            } else {\n                // For subsequent entries, just read the LFH header.\n                readFully(LFH_BUF);\n            }\n        } catch (final EOFException e) {\n            // Reached EOF cleanly, signaling end of archive.\n            return null;\n        }\n\n        final ZipLong sig = new ZipLong(LFH_BUF);\n\n        // Check if we've hit the Central Directory File Header (CFH) or Archive Extra Data Record (AED).\n        // These signify the end of the Local File Header entries.\n        if (sig.equals(ZipLong.CFH_SIG) || sig.equals(ZipLong.AED_SIG)) {\n            hitCentralDirectory = true;\n            // Skip the central directory structure. This is important so that\n            // subsequent operations on the stream don't try to read it as entry data.\n            skipRemainderOfArchive();\n            return null; // No more entries.\n        }\n\n        // Check if the signature is the expected Local File Header (LFH) signature.\n        if (!sig.equals(ZipLong.LFH_SIG)) {\n            // If the signature is not LFH, CFH, or AED, it indicates a corrupted archive\n            // or unexpected data where an entry was expected.\n            // Throw a ZipException instead of returning null.\n            throw new ZipException(String.format(\"Unexpected record signature: 0x%x\", sig.getValue()));\n        }\n\n        // --- Start processing the Local File Header ---\n        int off = WORD; // Start reading after the 4-byte signature.\n        current = new CurrentEntry(); // Holds state for the current entry being processed.\n\n        // Version needed to extract\n        final int versionMadeBy = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT;\n        // Extract platform information from the upper byte of versionMadeBy.\n        current.entry.setPlatform((versionMadeBy >> ZipFile.BYTE_SHIFT) & ZipFile.NIBLET_MASK);\n\n        // General purpose bit flag\n        final GeneralPurposeBit gpFlag = GeneralPurposeBit.parse(LFH_BUF, off);\n        final boolean hasUTF8Flag = gpFlag.usesUTF8ForNames();\n        // Determine the encoding for filename/comment based on the UTF-8 flag.\n        final ZipEncoding entryEncoding = hasUTF8Flag ? ZipEncodingHelper.UTF8_ZIP_ENCODING : zipEncoding;\n        // Check if a data descriptor is present after the compressed data.\n        current.hasDataDescriptor = gpFlag.usesDataDescriptor();\n        current.entry.setGeneralPurposeBit(gpFlag);\n        off += SHORT;\n\n        // Compression method\n        current.entry.setMethod(ZipShort.getValue(LFH_BUF, off));\n        off += SHORT;\n\n        // Last modification time and date\n        final long time = ZipUtil.dosToJavaTime(ZipLong.getValue(LFH_BUF, off));\n        current.entry.setTime(time);\n        off += WORD;\n\n        // CRC-32, compressed size, uncompressed size\n        ZipLong size = null, cSize = null;\n        if (!current.hasDataDescriptor) {\n            // If no data descriptor, these fields are in the LFH.\n            current.entry.setCrc(ZipLong.getValue(LFH_BUF, off));\n            off += WORD;\n            cSize = new ZipLong(LFH_BUF, off);\n            off += WORD;\n            size = new ZipLong(LFH_BUF, off);\n            off += WORD;\n        } else {\n            // If data descriptor is present, these fields in LFH are zero or placeholders.\n            // Skip these 12 bytes in the LFH buffer.\n            off += 3 * WORD;\n            // The actual values will be read later by closeEntry() or read() when the data descriptor is encountered.\n        }\n\n        // File name length\n        final int fileNameLen = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT;\n\n        // Extra field length\n        final int extraLen = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT; // 'off' now points to the start of the file name in the input stream.\n\n        // Read file name\n        final byte[] fileName = new byte[fileNameLen];\n        readFully(fileName);\n        // Decode file name using the determined encoding and store both raw and decoded names.\n        current.entry.setName(entryEncoding.decode(fileName), fileName);\n\n        // Read extra field data\n        final byte[] extraData = new byte[extraLen];\n        readFully(extraData);\n        current.entry.setExtra(extraData);\n\n        // If UTF-8 flag is not set but Unicode Path/Comment Extra Fields should be used,\n        // try to extract name and comment from those extra fields.\n        if (!hasUTF8Flag && useUnicodeExtraFields) {\n            ZipUtil.setNameAndCommentFromExtraFields(current.entry, fileName, null); // Pass null for comment bytes as they are not read here.\n        }\n\n        // Process Zip64 extra field if present. This might update CRC, sizes\n        // if they were set to 0xFFFFFFFF in the LFH, using values from the extra field.\n        // It uses the 'size' and 'cSize' read from LFH (or null if DD used) as potential\n        // indicators that Zip64 fields should be present.\n        processZip64Extra(size, cSize);\n\n        // --- Setup the input stream for reading entry data ---\n        // Note: This setup only prepares the stream wrapper if the compressed size is known *at this point*.\n        // If size is unknown (e.g., due to Data Descriptor), the stream setup might be deferred until the first read() call.\n        // The BoundedInputStream is crucial to prevent reading beyond the entry's data, especially in scenarios like nested archives.\n        if (current.entry.getCompressedSize() != ArchiveEntry.SIZE_UNKNOWN) {\n             // Only wrap with decompression/bounding streams if size is known.\n            long compressedSize = current.entry.getCompressedSize();\n            BoundedInputStream bounded = new BoundedInputStream(in, compressedSize);\n            if (current.entry.getMethod() == ZipMethod.UNSHRINKING.getCode()) {\n                current.in = new UnshrinkingInputStream(bounded);\n            } else if (current.entry.getMethod() == ZipMethod.IMPLODING.getCode()) {\n                current.in = new ExplodingInputStream(\n                        current.entry.getGeneralPurposeBit().getSlidingDictionarySize(),\n                        current.entry.getGeneralPurposeBit().getNumberOfShannonFanoTrees(),\n                        bounded);\n            } else if (current.entry.getMethod() == ZipMethod.BZIP2.getCode()) {\n                current.in = new BZip2CompressorInputStream(bounded);\n            }\n            // Assuming other methods like STORED and DEFLATED are handled similarly elsewhere\n            // or implicitly handled by the main read() method if current.in remains null or is set differently.\n            // (Handling for other methods like DEFLATED or STORED is assumed to be present in the complete class)\n        }\n        // If compressedSize is UNKNOWN, current.in might remain null. The read() method in ZipArchiveInputStream\n        // typically handles this case, potentially reading the Data Descriptor first if needed.\n\n        entriesRead++;\n        return current.entry;\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": true,
    "test": false
}