{
    "generation": "import java.net.URI;\nimport java.net.URISyntaxException;\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.StringTokenizer;\n// Assuming necessary imports for Logger, RobotToken, Directive, ParseState, SimpleRobotRules, RobotRulesMode etc. are present\n\n// Note: The percentEncodePath helper method is included for context, as it's essential for the fix.\n// Ideally, it would be a private static method within the same class as parseContent.\n\n/**\n * Percent-encodes a path string according to RFC 3986 for robots.txt path matching.\n * It encodes necessary characters into UTF-8 bytes and then percent-encodes bytes\n * that are not in the 'unreserved' set or the forward slash '/'.\n *\n * @param path The path string to encode.\n * @return The percent-encoded path string.\n */\nprivate static String percentEncodePath(String path) {\n    if (path == null) {\n        return null;\n    }\n    if (path.isEmpty()) {\n        return \"\";\n    }\n\n    StringBuilder sb = new StringBuilder();\n    // Encode the path using UTF-8 bytes\n    byte[] utf8Bytes = path.getBytes(StandardCharsets.UTF_8);\n\n    for (byte b : utf8Bytes) {\n        int unsignedByte = b & 0xFF;\n        // Check against unreserved characters from RFC 3986 Section 2.3\n        // unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n        if ((unsignedByte >= 'a' && unsignedByte <= 'z') ||\n            (unsignedByte >= 'A' && unsignedByte <= 'Z') ||\n            (unsignedByte >= '0' && unsignedByte <= '9') ||\n            unsignedByte == '-' || unsignedByte == '.' ||\n            unsignedByte == '_' || unsignedByte == '~') {\n            sb.append((char) unsignedByte);\n        } else if (unsignedByte == '/') {\n            // Do not encode the forward slash, keep it as is for path structure.\n            sb.append('/');\n        } else {\n            // Percent-encode all other bytes.\n            sb.append(String.format(\"%%%02X\", unsignedByte));\n        }\n    }\n    return sb.toString();\n}\n\n\nprivate SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n    // If there's nothing there, treat it like we have no restrictions.\n    if ((content == null) || (content.length == 0)) {\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n    }\n\n    int bytesLen = content.length;\n    int offset = 0;\n    Charset encoding = StandardCharsets.US_ASCII;\n\n    // Check for a UTF-8 BOM at the beginning (EF BB BF)\n    if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n        offset = 3;\n        bytesLen -= 3;\n        encoding = StandardCharsets.UTF_8;\n    }\n    // Check for UTF-16LE BOM at the beginning (FF FE)\n    else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16LE;\n    }\n    // Check for UTF-16BE BOM at the beginning (FE FF)\n    else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16BE;\n    }\n\n    String contentAsStr;\n    contentAsStr = new String(content, offset, bytesLen, encoding);\n\n    // Decide if we need to do special HTML processing.\n    boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n    // If it looks like it contains HTML, but doesn't have a user agent\n    // field, then\n    // assume somebody messed up and returned back to us a random HTML page\n    // instead\n    // of a robots.txt file.\n    boolean hasHTML = false;\n    if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n        if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n            LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        } else {\n            // We'll try to strip out HTML tags below.\n            if (isHtmlType) {\n                LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n            } else {\n                LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n            }\n\n            hasHTML = true;\n        }\n    }\n\n    // Break on anything that might be used as a line ending. Since\n    // tokenizer doesn't return empty tokens, a \\r\\n sequence still\n    // works since it looks like an empty string between the \\r and \\n.\n    StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n    ParseState parseState = new ParseState(url, robotNames);\n\n    while (lineParser.hasMoreTokens()) {\n        String line = lineParser.nextToken();\n\n        // Get rid of HTML markup, in case some brain-dead webmaster has\n        // created an HTML\n        // page for robots.txt. We could do more sophisticated processing\n        // here to better\n        // handle bad HTML, but that's a very tiny percentage of all\n        // robots.txt files.\n        if (hasHTML) {\n            line = line.replaceAll(\"<[^>]+>\", \"\");\n        }\n\n        // trim out comments and whitespace\n        int hashPos = line.indexOf(\"#\");\n        if (hashPos >= 0) {\n            line = line.substring(0, hashPos);\n        }\n\n        line = line.trim();\n        if (line.length() == 0) {\n            continue;\n        }\n\n        RobotToken token = tokenize(line);\n        // Use a potentially modified token for handlers that require encoded data.\n        // This assumes RobotToken can be instantiated with new data, e.g., new RobotToken(directive, data).\n        // If RobotToken is immutable without such a constructor, this approach needs adjustment.\n        RobotToken potentiallyModifiedToken = token;\n\n        // RFC 9309 requires path values for Allow/Disallow to be compared after percent-encoding.\n        // We encode the path here before passing it to the handler, storing the canonical form.\n        if (token.getDirective() == Directive.ALLOW || token.getDirective() == Directive.DISALLOW) {\n             String originalPath = token.getData();\n             if (originalPath != null) {\n                 String encodedPath = percentEncodePath(originalPath);\n                 // Assume RobotToken can be created with updated data.\n                 potentiallyModifiedToken = new RobotToken(token.getDirective(), encodedPath);\n             }\n        }\n        // Sitemap URLs should also be handled correctly, potentially involving percent-encoding.\n        // We use Java's URI class for robust handling of full URLs including IDN.\n        else if (token.getDirective() == Directive.SITEMAP) {\n             String sitemapUrlStr = token.getData();\n             if (sitemapUrlStr != null) {\n                 try {\n                     URI sitemapUri = new URI(sitemapUrlStr);\n                     // toASCIIString handles IDN and ensures proper percent-encoding.\n                     String encodedSitemapUrlStr = sitemapUri.toASCIIString();\n                     potentiallyModifiedToken = new RobotToken(token.getDirective(), encodedSitemapUrlStr);\n                 } catch (URISyntaxException e) {\n                     reportWarning(parseState, \"Invalid Sitemap URL syntax: {}\", sitemapUrlStr);\n                     // Keep original token if invalid; handler might skip or log.\n                     potentiallyModifiedToken = token;\n                 }\n             }\n        }\n\n\n        // Use the potentially modified token for relevant directives in the switch.\n        switch (potentiallyModifiedToken.getDirective()) {\n            case USER_AGENT:\n            // User agent names typically don't require URL encoding. Pass original token.\n            handleUserAgent(parseState, token);\n                break;\n\n            case DISALLOW:\n            parseState.setFinishedAgentFields(true);\n            // Pass the token with the percent-encoded path.\n            handleDisallow(parseState, potentiallyModifiedToken);\n                break;\n\n            case ALLOW:\n            parseState.setFinishedAgentFields(true);\n            // Pass the token with the percent-encoded path.\n            handleAllow(parseState, potentiallyModifiedToken);\n                break;\n\n            case CRAWL_DELAY:\n            parseState.setFinishedAgentFields(true);\n            // Crawl-delay value is numeric, no encoding needed. Pass original token.\n            handleCrawlDelay(parseState, token);\n                break;\n\n            case SITEMAP:\n            parseState.setFinishedAgentFields(true);\n            // Pass the potentially modified token with the encoded/validated Sitemap URL.\n            handleSitemap(parseState, potentiallyModifiedToken);\n                break;\n\n            case HTTP: // Directive not standard; pass original token.\n            parseState.setFinishedAgentFields(true);\n            handleHttp(parseState, token);\n                break;\n\n            case UNKNOWN:\n            // Use the original line for the warning message.\n            reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n            parseState.setFinishedAgentFields(true);\n                break;\n\n            case MISSING:\n             // Use the original line for the warning message.\n            reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n            parseState.setFinishedAgentFields(true);\n                break;\n\n            default:\n                // All others we just ignore (e.g., Host, NoIndex, Request-Rate, Visit-Time if defined)\n                // Existing TODOs apply. Consider if finishedAgentFields should be set for these.\n                break;\n        }\n    }\n\n    this._numWarningsDuringLastParse.set(parseState._numWarnings);\n    SimpleRobotRules result = parseState.getRobotRules();\n    if (result.getCrawlDelay() > _maxCrawlDelay) {\n        // Some evil sites use a value like 3600 (seconds) for the crawl\n        // delay, which would cause lots of problems for us.\n        LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n    } else {\n        result.sortRules();\n        return result;\n    }\n}",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}