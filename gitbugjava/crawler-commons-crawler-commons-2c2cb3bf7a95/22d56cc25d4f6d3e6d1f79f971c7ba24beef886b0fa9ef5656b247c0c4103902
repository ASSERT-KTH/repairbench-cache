{
    "generation": "    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        // Per RFC 9309, the default encoding is UTF-8.\n        Charset encoding = StandardCharsets.UTF_8;\n\n        // Check for a UTF-8 BOM at the beginning (EF BB BF)\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            // encoding remains StandardCharsets.UTF_8;\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n        // No other BOMs are standardly recognized for robots.txt. Stick with UTF-8 default if no BOM.\n\n        String contentAsStr;\n        try {\n            contentAsStr = new String(content, offset, bytesLen, encoding);\n        } catch (UnsupportedOperationException | IllegalArgumentException e) {\n            // This can happen if the detected encoding results in invalid characters\n            // or if the byte sequence is invalid for the encoding.\n            // Treat as unparsable, allow all for safety.\n            LOGGER.warn(\"Failed to decode robots.txt content from {} using encoding {}. Allowing all.\", url, encoding, e);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        // Decide if we need to do special HTML processing.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        // If it looks like it contains HTML, but doesn't have a user agent\n        // field, then assume somebody messed up and returned back to us a random HTML page\n        // instead of a robots.txt file.\n        boolean hasHTML = false;\n        // Optimization: Check for user-agent first before doing expensive HTML regex matching.\n        Matcher userAgentMatcher = USER_AGENT_PATTERN.matcher(contentAsStr);\n        boolean hasUserAgent = userAgentMatcher.find(); // Find the first occurrence\n\n        // Consider it HTML if content type is HTML, OR if no User-Agent is found AND it contains HTML tags.\n        if (isHtmlType || (!hasUserAgent && SIMPLE_HTML_PATTERN.matcher(contentAsStr).find())) {\n            // If it doesn't have a User-Agent line, it's likely not a valid robots.txt file.\n            if (!hasUserAgent) {\n                LOGGER.trace(\"Found non-robots.txt HTML file (no user-agent): \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // It has a User-Agent line, but also HTML characteristics.\n                // Proceed with parsing, but enable HTML stripping.\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n                } else {\n                    // HTML tags detected even without HTML content type\n                    LOGGER.debug(\"Found HTML markup in robots.txt file: \" + url);\n                }\n                hasHTML = true;\n            }\n        }\n\n        // Break on anything that might be used as a line ending. Since\n        // tokenizer doesn't return empty tokens, a \\r\\n sequence still\n        // works since it looks like an empty string between the \\r and \\n.\n        // Using specific newline chars from RFC and common practice.\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        ParseState parseState = new ParseState(url, robotNames);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            // Get rid of HTML markup, in case some brain-dead webmaster has\n            // created an HTML page for robots.txt.\n            if (hasHTML) {\n                // Basic check to avoid regex on lines without tags\n                if (line.indexOf('<') != -1) {\n                    line = line.replaceAll(\"<[^>]+>\", \"\");\n                }\n            }\n\n            // trim out comments and whitespace\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            line = line.trim();\n            if (line.length() == 0) {\n                continue;\n            }\n\n            // Assuming tokenize and handlers correctly process the decoded line.\n            // Specifically, path directives (Allow/Disallow) might need percent-encoding,\n            // ideally handled within the respective handlers or ParseState/SimpleRobotRules\n            // based on RFC 9309 recommendations for matching against percent-encoded URLs.\n            // This function ensures the line is correctly decoded using UTF-8 by default.\n            RobotToken token = tokenize(line);\n            switch (token.getDirective()) {\n                case USER_AGENT:\n                handleUserAgent(parseState, token);\n                    break;\n\n                case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, token);\n                    break;\n\n                case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, token);\n                    break;\n\n                case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, token);\n                    break;\n\n                case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, token);\n                    break;\n\n                case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, token);\n                    break;\n\n                case UNKNOWN:\n                // An unrecognized directive (e.g., \"Foo: bar\")\n                reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                case MISSING:\n                // A line that doesn't match \"directive: value\" (e.g., \"just some text\")\n                reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                default:\n                    // Includes directives like NOINDEX, etc., which are currently ignored.\n                    // Consider if ignored directives should mark the end of a user-agent record.\n                    // parseState.setFinishedAgentFields(true);\n                    break;\n            }\n        }\n\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        SimpleRobotRules result = parseState.getRobotRules();\n        \n        // Apply crawl-delay limit check *before* sorting, as disallowed rules aren't sorted.\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            // Some evil sites use a value like 3600 (seconds) for the crawl\n            // delay, which would cause lots of problems for us.\n            LOGGER.debug(\"Crawl delay ({}) exceeds max value ({}) - disallowing all URLs: {}\", result.getCrawlDelay(), _maxCrawlDelay, url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n            // Sort rules for efficient matching (e.g., longest path first)\n            result.sortRules();\n            return result;\n        }\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}