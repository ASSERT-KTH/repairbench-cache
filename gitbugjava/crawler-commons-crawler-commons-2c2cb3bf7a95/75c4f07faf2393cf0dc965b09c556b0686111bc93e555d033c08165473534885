{
    "generation": "    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        Charset encoding = StandardCharsets.US_ASCII;\n\n        // Check for BOMs\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            encoding = StandardCharsets.UTF_8;\n        } else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        } else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n\n        // Check if there are any non-ASCII bytes (`0x80` or above)\n        boolean hasNonASCII = false;\n        for (int i = offset; i < content.length; i++) {\n            if ((content[i] & 0x80) != 0) {\n                hasNonASCII = true;\n                break;\n            }\n        }\n\n        // Switch to Latin-1 if the content contains non-ASCII characters and we\u2019re still in US-ASCII mode\n        if (encoding.equals(StandardCharsets.US_ASCII) && hasNonASCII) {\n            encoding = StandardCharsets.ISO_8859_1;\n        }\n\n        // Decode the content with the final detected encoding\n        String contentAsStr = new String(content, offset, bytesLen, encoding);\n\n        // Continue parsing...\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n        boolean hasHTML = false;\n\n        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n                LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // HTML tag stripping logic...\n                hasHTML = true;\n            }\n        }\n\n        // Remainder of parsing logic (unchanged)\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        ParseState parseState = new ParseState(url, robotNames);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            if (hasHTML) {\n                line = line.replaceAll(\"<[^>]+>\", \"\");\n            }\n\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            line = line.trim();\n            if (line.isEmpty()) {\n                continue;\n            }\n\n            RobotToken token = tokenize(line);\n            switch (token.getDirective()) {\n                // Handled cases remain unchanged...\n                default:\n                    break;\n            }\n        }\n\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        SimpleRobotRules result = parseState.getRobotRules();\n\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n            result.sortRules();\n            return result;\n        }\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}