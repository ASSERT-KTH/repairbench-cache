{
    "generation": "private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n    if ((content == null) || (content.length == 0)) {\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n    }\n\n    int bytesLen = content.length;\n    int offset = 0;\n    Charset encoding = StandardCharsets.US_ASCII;\n\n    // Check for BOMs\n    boolean hasBom = false;\n    if (bytesLen >= 3 && content[0] == (byte) 0xEF && content[1] == (byte) 0xBB && content[2] == (byte) 0xBF) {\n        offset = 3;\n        bytesLen -= 3;\n        encoding = StandardCharsets.UTF_8;\n        hasBom = true;\n    } else if (bytesLen >= 2 && content[0] == (byte) 0xFF && content[1] == (byte) 0xFE) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16LE;\n        hasBom = true;\n    } else if (bytesLen >= 2 && content[0] == (byte) 0xFE && content[1] == (byte) 0xFF) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16BE;\n        hasBom = true;\n    }\n\n    // If no BOM was found, try parsing charset from Content-Type\n    if (!hasBom && contentType != null) {\n        // Normalize content-type\n        String lowerContentType = contentType.toLowerCase(Locale.ROOT);\n\n        int charsetStart = lowerContentType.indexOf(\"charset=\");\n        if (charsetStart != -1) {\n            // Find end index of charset\n            charsetStart += 8;\n            int charsetEnd = contentType.length();\n            for (int i = charsetStart; i < contentType.length(); i++) {\n                char c = contentType.charAt(i);\n                if (c == ' ' || c == ';' || c == ',') {\n                    charsetEnd = i;\n                    break;\n                }\n            }\n\n            if (charsetEnd > charsetStart) {\n                String charsetName = contentType.substring(charsetStart, charsetEnd).trim();\n                try {\n                    encoding = Charset.forName(charsetName);\n                } catch (Exception ignore) {\n                    // ignore invalid charsets\n                }\n            }\n        }\n    }\n\n    // Decode the content using the chosen encoding\n    String contentAsStr = new String(content, offset, bytesLen, encoding);\n\n    boolean isHtmlType = (contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\");\n\n    boolean hasHTML = false;\n    if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n        if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n            LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        } else {\n            if (isHtmlType) {\n                LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n            } else {\n                LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n            }\n            hasHTML = true;\n        }\n    }\n\n    StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n    ParseState parseState = new ParseState(url, robotNames);\n\n    while (lineParser.hasMoreTokens()) {\n        String line = lineParser.nextToken();\n\n        if (hasHTML) {\n            line = line.replaceAll(\"<[^>]+>\", \"\");\n        }\n\n        int hashPos = line.indexOf(\"#\");\n        if (hashPos >= 0) {\n            line = line.substring(0, hashPos);\n        }\n\n        line = line.trim();\n        if (line.length() == 0) {\n            continue;\n        }\n\n        RobotToken originalToken = tokenize(line);\n\n        // URL decode path before handling\n        RobotToken decodedToken;\n        try {\n            decodedToken = decodeUnicodeToken(originalToken);\n        } catch (Exception ignore) {\n            decodedToken = originalToken;\n        }\n\n        switch (decodedToken.getDirective()) {\n            case USER_AGENT:\n                handleUserAgent(parseState, decodedToken);\n                break;\n            case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, decodedToken);\n                break;\n            case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, decodedToken);\n                break;\n            case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, decodedToken);\n                break;\n            case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, decodedToken);\n                break;\n            case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, decodedToken);\n                break;\n            case UNKNOWN:\n                reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                break;\n            case MISSING:\n                reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                parseState.setFinishedAgentFields(true);\n                break;\n            default:\n                break;\n        }\n    }\n\n    this._numWarningsDuringLastParse.set(parseState._numWarnings);\n    SimpleRobotRules result = parseState.getRobotRules();\n\n    if (result.getCrawlDelay() > _maxCrawlDelay) {\n        LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n    } else {\n        result.sortRules();\n        return result;\n    }\n}",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}