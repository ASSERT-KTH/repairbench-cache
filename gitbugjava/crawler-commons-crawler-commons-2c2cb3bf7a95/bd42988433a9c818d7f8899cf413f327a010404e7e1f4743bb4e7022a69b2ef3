{
    "generation": "private SimpleRobotRules parseContent(String url, byte[] content, String contentType,\n                                     Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n    // If there's nothing there, treat it like we have no restrictions.\n    if (content == null || content.length == 0) {\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n    }\n\n    int bytesLen = content.length;\n    int offset = 0;\n    // Default to UTF-8 (so we correctly handle unescaped Unicode by default)\n    Charset encoding = StandardCharsets.UTF_8;\n\n    // Check for a UTF-8 BOM at the beginning (EF BB BF)\n    if (bytesLen >= 3\n        && content[0] == (byte) 0xEF\n        && content[1] == (byte) 0xBB\n        && content[2] == (byte) 0xBF) {\n        offset = 3;\n        bytesLen -= 3;\n        encoding = StandardCharsets.UTF_8;\n    }\n    // Check for UTF-16LE BOM at the beginning (FF FE)\n    else if (bytesLen >= 2\n             && content[0] == (byte) 0xFF\n             && content[1] == (byte) 0xFE) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16LE;\n    }\n    // Check for UTF-16BE BOM at the beginning (FE FF)\n    else if (bytesLen >= 2\n             && content[0] == (byte) 0xFE\n             && content[1] == (byte) 0xFF) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16BE;\n    }\n\n    String contentAsStr = new String(content, offset, bytesLen, encoding);\n\n    // Decide if we need to do special HTML processing.\n    boolean isHtmlType = (contentType != null\n                          && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n    // If it looks like HTML but has no \"User-agent\", assume not a robots.txt\n    boolean hasHTML = false;\n    if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n        if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n            LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        } else {\n            if (isHtmlType) {\n                LOGGER.debug(\"HTML content type returned for robots.txt file: {}\", url);\n            } else {\n                LOGGER.debug(\"Found HTML in robots.txt file: {}\", url);\n            }\n            hasHTML = true;\n        }\n    }\n\n    // Split into lines on any newline-type character\n    StringTokenizer lineParser =\n        new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n    ParseState parseState = new ParseState(url, robotNames);\n\n    while (lineParser.hasMoreTokens()) {\n        String line = lineParser.nextToken();\n        if (hasHTML) {\n            line = line.replaceAll(\"<[^>]+>\", \"\");\n        }\n        int hashPos = line.indexOf('#');\n        if (hashPos >= 0) {\n            line = line.substring(0, hashPos);\n        }\n        line = line.trim();\n        if (line.isEmpty()) {\n            continue;\n        }\n\n        RobotToken token = tokenize(line);\n        switch (token.getDirective()) {\n            case USER_AGENT:\n                handleUserAgent(parseState, token);\n                break;\n            case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, token);\n                break;\n            case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, token);\n                break;\n            case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, token);\n                break;\n            case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, token);\n                break;\n            case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, token);\n                break;\n            case UNKNOWN:\n                reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                break;\n            case MISSING:\n                reportWarning(parseState,\n                              \"Unknown line in robots.txt file (size {}): {}\",\n                              content.length, line);\n                parseState.setFinishedAgentFields(true);\n                break;\n            default:\n                // ignore other directives\n                break;\n        }\n    }\n\n    this._numWarningsDuringLastParse.set(parseState._numWarnings);\n    SimpleRobotRules result = parseState.getRobotRules();\n\n    //\n    // === NEW LOGIC to handle Unicode in Disallow/Allow paths ===\n    // For each parsed rule, add the percent\u2010encoded variant if it contains\n    // raw Unicode, and add the Unicode variant if it contains percent escapes.\n    //\n    java.util.List<SimpleRobotRules.RobotRule> originalRules =\n        new java.util.ArrayList<>(result.getRules());\n    for (SimpleRobotRules.RobotRule rule : originalRules) {\n        String path = rule.path;\n        boolean allowRule = rule.allow;\n\n        // 1) If the rule contains percent-escapes, decode them and add that variant\n        if (path.indexOf('%') >= 0) {\n            java.io.ByteArrayOutputStream baos = new java.io.ByteArrayOutputStream();\n            for (int i = 0; i < path.length(); ) {\n                char c = path.charAt(i);\n                if (c == '%' && i + 2 < path.length()) {\n                    char c1 = path.charAt(i + 1), c2 = path.charAt(i + 2);\n                    boolean hex1 = (c1 >= '0' && c1 <= '9')\n                                   || (c1 >= 'A' && c1 <= 'F')\n                                   || (c1 >= 'a' && c1 <= 'f');\n                    boolean hex2 = (c2 >= '0' && c2 <= '9')\n                                   || (c2 >= 'A' && c2 <= 'F')\n                                   || (c2 >= 'a' && c2 <= 'f');\n                    if (hex1 && hex2) {\n                        int b = Integer.parseInt(path.substring(i + 1, i + 3), 16);\n                        baos.write(b);\n                        i += 3;\n                        continue;\n                    }\n                }\n                baos.write((byte) c);\n                i++;\n            }\n            String decoded = new String(baos.toByteArray(), StandardCharsets.UTF_8);\n            if (!decoded.equals(path)) {\n                if (allowRule) {\n                    result.addAllow(decoded);\n                } else {\n                    result.addDisallow(decoded);\n                }\n            }\n        }\n\n        // 2) If the rule contains raw non-ASCII, percent-encode it and add that variant\n        boolean hasNonAscii = false;\n        for (int j = 0; j < path.length(); ) {\n            int cp = path.codePointAt(j);\n            if (cp > 0x7F) {\n                hasNonAscii = true;\n                break;\n            }\n            j += Character.charCount(cp);\n        }\n        if (hasNonAscii) {\n            StringBuilder sb = new StringBuilder();\n            for (int i = 0; i < path.length(); ) {\n                int cp = path.codePointAt(i);\n                if (cp <= 0x7F) {\n                    sb.append((char) cp);\n                } else {\n                    byte[] bytes = new String(Character.toChars(cp))\n                                   .getBytes(StandardCharsets.UTF_8);\n                    for (byte b : bytes) {\n                        sb.append(String.format(\"%%%02X\", b & 0xFF));\n                    }\n                }\n                i += Character.charCount(cp);\n            }\n            String encoded = sb.toString();\n            if (!encoded.equals(path)) {\n                if (allowRule) {\n                    result.addAllow(encoded);\n                } else {\n                    result.addDisallow(encoded);\n                }\n            }\n        }\n    }\n\n    // Enforce max crawl-delay if any\n    if (result.getCrawlDelay() > _maxCrawlDelay) {\n        LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n    } else {\n        result.sortRules();\n        return result;\n    }\n}",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}