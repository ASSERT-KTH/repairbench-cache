{
    "generation": "import java.nio.ByteBuffer;\nimport java.nio.charset.CharacterCodingException;\nimport java.nio.charset.Charset;\nimport java.nio.charset.CharsetDecoder;\nimport java.nio.charset.CodingErrorAction;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.StringTokenizer;\n\n// Assuming other necessary classes like SimpleRobotRules, RobotRulesMode, RobotToken, ParseState, LOGGER,\n// SIMPLE_HTML_PATTERN, USER_AGENT_PATTERN, _numWarningsDuringLastParse, _maxCrawlDelay,\n// tokenize, handleUserAgent, handleDisallow, handleAllow, handleCrawlDelay, handleSitemap,\n// handleHttp, reportWarning are available in the scope.\n\nclass YourClassName { // Replace YourClassName with the actual class name containing the function\n\n    // Assuming these are defined elsewhere in the class or accessible static members\n    // private static final Logger LOGGER = LoggerFactory.getLogger(YourClassName.class);\n    // private static final Pattern SIMPLE_HTML_PATTERN = Pattern.compile(\"<html\", Pattern.CASE_INSENSITIVE);\n    // private static final Pattern USER_AGENT_PATTERN = Pattern.compile(\"user-agent\", Pattern.CASE_INSENSITIVE);\n    // private final AtomicInteger _numWarningsDuringLastParse = new AtomicInteger(0);\n    // private int _maxCrawlDelay = 3600; // Example default, adjust as per actual class definition\n\n    // Mock/placeholder for missing context, to make the snippet runnable if standalone\n    // Replace with actual imports/definitions from the original project if providing a full class.\n    static class SimpleRobotRules {\n        enum RobotRulesMode { ALLOW_ALL, ALLOW_NONE }\n        public SimpleRobotRules(RobotRulesMode mode) {}\n        public SimpleRobotRules() {}\n        public long getCrawlDelay() { return 0; }\n        public void sortRules() {}\n        public boolean isAllowed(String url) { return true; } // Mock for compilation\n    }\n    static class RobotToken {\n        enum Directive { USER_AGENT, DISALLOW, ALLOW, CRAWL_DELAY, SITEMAP, HTTP, UNKNOWN, MISSING }\n        public Directive getDirective() { return Directive.UNKNOWN; }\n        public String getAgent() { return \"\"; }\n        public String getPath() { return \"\"; }\n        public String getUrl() { return \"\"; }\n        public long getCrawlDelay() { return 0; }\n    }\n    static class ParseState {\n        int _numWarnings = 0;\n        public ParseState(String url, Collection<String> robotNames) {}\n        public void setFinishedAgentFields(boolean finished) {}\n        public SimpleRobotRules getRobotRules() { return new SimpleRobotRules(); }\n    }\n    // Assume LOGGER is available from a logging framework like SLF4J\n    static class LOGGER {\n        static void trace(String msg) {}\n        static void debug(String msg) {}\n        static void debug(String msg, Object... args) {}\n    }\n    static java.util.regex.Pattern SIMPLE_HTML_PATTERN = java.util.regex.Pattern.compile(\"<html\", java.util.regex.Pattern.CASE_INSENSITIVE);\n    static java.util.regex.Pattern USER_AGENT_PATTERN = java.util.regex.Pattern.compile(\"user-agent\", java.util.regex.Pattern.CASE_INSENSITIVE);\n    private final java.util.concurrent.atomic.AtomicInteger _numWarningsDuringLastParse = new java.util.concurrent.atomic.AtomicInteger(0);\n    private int _maxCrawlDelay = 3600;\n\n    private RobotToken tokenize(String line) { return new RobotToken(); } // Mock\n    private void handleUserAgent(ParseState parseState, RobotToken token) {} // Mock\n    private void handleDisallow(ParseState parseState, RobotToken token) {} // Mock\n    private void handleAllow(ParseState parseState, RobotToken token) {} // Mock\n    private void handleCrawlDelay(ParseState parseState, RobotToken token) {} // Mock\n    private void handleSitemap(ParseState parseState, RobotToken token) {} // Mock\n    private void handleHttp(ParseState parseState, RobotToken token) {} // Mock\n    private void reportWarning(ParseState parseState, String format, Object... args) {} // Mock\n\n    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        Charset encoding = null; // Start with null, will be determined by BOM or fallback\n\n        // Check for a UTF-8 BOM at the beginning (EF BB BF)\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            encoding = StandardCharsets.UTF_8;\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n\n        String contentAsStr;\n        // If no BOM was found, try common encodings. RFC 9309 mandates UTF-8.\n        if (encoding == null) {\n            // First try UTF-8, as it's the standard and most common for non-ASCII.\n            try {\n                CharsetDecoder decoder = StandardCharsets.UTF_8.newDecoder()\n                        .onMalformedInput(CodingErrorAction.REPORT)\n                        .onUnmappableCharacter(CodingErrorAction.REPORT);\n                contentAsStr = decoder.decode(ByteBuffer.wrap(content, offset, bytesLen)).toString();\n                encoding = StandardCharsets.UTF_8; // Mark that UTF-8 was successfully used\n            } catch (CharacterCodingException e) {\n                // If UTF-8 fails (malformed input), try ISO-8859-1 as a common fallback for legacy European content.\n                try {\n                    CharsetDecoder decoder = StandardCharsets.ISO_8859_1.newDecoder()\n                            .onMalformedInput(CodingErrorAction.REPORT)\n                            .onUnmappableCharacter(CodingErrorAction.REPORT);\n                    contentAsStr = decoder.decode(ByteBuffer.wrap(content, offset, bytesLen)).toString();\n                    encoding = StandardCharsets.ISO_8859_1; // Mark that ISO-8859-1 was successfully used\n                } catch (CharacterCodingException e2) {\n                    // If all strict decodings fail, fall back to US_ASCII and let it replace bad characters.\n                    // This is the least preferred but most robust fallback, ensuring parsing continues.\n                    LOGGER.debug(\"Could not decode robots.txt with UTF-8 or ISO-8859-1; falling back to US-ASCII (replacement characters will be used): {}\", url);\n                    contentAsStr = new String(content, offset, bytesLen, StandardCharsets.US_ASCII);\n                    encoding = StandardCharsets.US_ASCII; // Mark the fallback encoding\n                }\n            }\n        } else {\n            // A BOM was detected, use that encoding directly.\n            // The String constructor will replace malformed characters if any, which is acceptable behavior\n            // as BOMs are strong indicators of the intended encoding.\n            contentAsStr = new String(content, offset, bytesLen, encoding);\n        }\n\n        // Decide if we need to do special HTML processing.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        // If it looks like it contains HTML, but doesn't have a user agent\n        // field, then\n        // assume somebody messed up and returned back to us a random HTML page\n        // instead\n        // of a robots.txt file.\n        boolean hasHTML = false;\n        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n                LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // We'll try to strip out HTML tags below.\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n                } else {\n                    LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n                }\n\n                hasHTML = true;\n            }\n        }\n\n        // Break on anything that might be used as a line ending. Since\n        // tokenizer doesn't return empty tokens, a \\r\\n sequence still\n        // works since it looks like an empty string between the \\r and \\n.\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        ParseState parseState = new ParseState(url, robotNames);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            // Get rid of HTML markup, in case some brain-dead webmaster has\n            // created an HTML\n            // page for robots.txt. We could do more sophisticated processing\n            // here to better\n            // handle bad HTML, but that's a very tiny percentage of all\n            // robots.txt files.\n            if (hasHTML) {\n                line = line.replaceAll(\"<[^>]+>\", \"\");\n            }\n\n            // trim out comments and whitespace\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            line = line.trim();\n            if (line.length() == 0) {\n                continue;\n            }\n\n            RobotToken token = tokenize(line);\n            switch (token.getDirective()) {\n                case USER_AGENT:\n                handleUserAgent(parseState, token);\n                    break;\n\n                case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, token);\n                    break;\n\n                case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, token);\n                    break;\n\n                case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, token);\n                    break;\n\n                case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, token);\n                    break;\n\n                case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, token);\n                    break;\n\n                case UNKNOWN:\n                reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                case MISSING:\n                reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                default:\n                    // All others we just ignore\n                    // TODO KKr - which of these should be setting\n                    // finishedAgentFields to true?\n                    // TODO KKr - handle no-index\n                    // TODO KKr - handle request-rate and visit-time\n                    break;\n            }\n        }\n\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        SimpleRobotRules result = parseState.getRobotRules();\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            // Some evil sites use a value like 3600 (seconds) for the crawl\n            // delay, which would cause lots of problems for us.\n            LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n            result.sortRules();\n            return result;\n        }\n    }\n}",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}