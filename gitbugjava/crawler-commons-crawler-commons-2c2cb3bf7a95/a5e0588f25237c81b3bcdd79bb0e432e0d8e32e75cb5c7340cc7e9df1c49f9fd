{
    "generation": "private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n    // If there's nothing there, treat it like we have no restrictions.\n    if ((content == null) || (content.length == 0)) {\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n    }\n\n    int bytesLen = content.length;\n    int offset = 0;\n    Charset encoding = StandardCharsets.UTF_8; // Fixed: Changed default to UTF-8 per robots.txt specs.\n\n    // Check for a UTF-8 BOM at the beginning (EF BB BF). If absent, robots.txt should be UTF-8.\n    if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n        offset = 3;\n        bytesLen -= 3;\n        // Still retain encoding as UTF_8 in this case so in the end it's just a no-op (or required? Previously, the default):\n        // Wait, actually no, the BOM detection for UTF-8 is unnecessary since we already default to UTF-8. Alternatively, the code left as is is okay, since it keeps UTF-8 encoding after consuming BOM (which is redundant, but safe). \n\n    } else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16LE;\n    } else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16BE;\n    }\n    // We have to read the bytes as the correct encoding per the BOM or default to UTF-8.\n\n    String contentAsStr = new String(content, offset, bytesLen, encoding);\n\n    // The rest remains unchanged. The tokenizer and parsing logic now work on the correctly decoded UTF-8 string as per the robots.txt specification.\n    ...\n}",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}