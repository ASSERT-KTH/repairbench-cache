{
    "generation": "    private String canonicalizePath(String path, String urlForLoggingContext) {\n        if (path == null) {\n            // This case should ideally not be reached if called with token.getValue()\n            return null;\n        }\n        \n        // In URI paths, '+' should be treated as a literal character, not a space.\n        // URLDecoder.decode treats '+' as a space, so we temporarily replace '+' with \"%2B\"\n        // to ensure it's decoded back to a literal '+' character.\n        String pathForDecoder = path.replace(\"+\", \"%2B\");\n        String decodedPath;\n\n        try {\n            decodedPath = URLDecoder.decode(pathForDecoder, StandardCharsets.UTF_8.name());\n        } catch (UnsupportedEncodingException e) {\n            // This should never happen as UTF-8 is a standard charset\n            throw new RuntimeException(\"UTF-8 not supported for URL decoding\", e);\n        } catch (IllegalArgumentException e) {\n            // This exception occurs if the path string contains malformed percent-encoded sequences (e.g., \"%\", \"%XG\").\n            // Such a rule is likely invalid or unintended. We log a warning and skip the rule.\n            LOGGER.warn(\"Path '{}' in robots.txt for url '{}' contains invalid percent-encoding. Rule will be ignored.\", path, urlForLoggingContext);\n            return null; \n        }\n\n        // Re-encode the decoded path to a canonical, UTF-8 percent-encoded form.\n        try {\n            // Using new URI(scheme, ssp, fragment) with null scheme/ssp and path as fragment \n            // is a way to get the path encoded. Or specifically for path component:\n            URI uri = new URI(null, null, decodedPath, null, null);\n            String encodedPath = uri.getRawPath(); // getRawPath returns the path properly percent-encoded.\n            \n            // If decodedPath was null, getRawPath might be null. But we checked `path` at the start.\n            // If decodedPath is empty, getRawPath returns an empty string.\n            // This is handled by addRule logic (empty path often becomes \"/\").\n            return encodedPath;\n        } catch (URISyntaxException e) {\n            // This exception can occur if the decodedPath, even after URL decoding,\n            // contains characters that are not permissible in a URI path component\n            // (e.g., certain control characters or structural issues not caught by URLDecoder).\n            LOGGER.warn(\"Path '{}' (decoded from '{}') in robots.txt for url '{}' could not be URI-encoded due to invalid syntax. Rule will be ignored.\", decodedPath, path, urlForLoggingContext);\n            return null;\n        }\n    }\n\n    private void handleDisallow(ParseState parseState, RobotToken token) {\n        if (parseState.isCurGroupMatched()) {\n            String rawPath = token.getValue();\n            if (rawPath.length() == 0) {\n                // Current behavior: \"Disallow: \" (empty path) is ignored.\n                // This means an empty Disallow rule doesn't disallow anything by itself.\n                // (Google's spec says empty path is equivalent to \"/\", but we preserve existing behavior here).\n            } else {\n                String canonicalPath = canonicalizePath(rawPath, parseState.getUrl());\n                if (canonicalPath != null) { // If canonicalization succeeded\n                    parseState.addDisallowRule(canonicalPath);\n                }\n                // If canonicalPath is null, it means the path was malformed; a warning\n                // was logged by canonicalizePath, and the rule is effectively ignored.\n            }\n        }\n    }\n\n    private void handleAllow(ParseState parseState, RobotToken token) {\n        if (parseState.isCurGroupMatched()) {\n            String rawPath = token.getValue();\n            // For \"Allow: \" (empty path), rawPath is \"\".\n            // canonicalizePath(\"\") returns \"\".\n            // parseState.addAllowRule(\"\") typically results in a rule for \"/\"\n            // due to logic in SimpleRobotRules.addRule (prepending \"/\" if missing and not empty,\n            // or specific handling for \"\" to mean \"/\"). This aligns with Google's spec.\n            String canonicalPath = canonicalizePath(rawPath, parseState.getUrl());\n            if (canonicalPath != null) { // If canonicalization succeeded\n                parseState.addAllowRule(canonicalPath);\n            }\n            // If canonicalPath is null, rule is ignored (warning logged by canonicalizePath).\n        }\n    }\n\n    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        // NOTE: The _defaultEncoding field of SimpleRobotRulesParser (typically UTF-8) should be used\n        // here if no BOM is found. The original code defaults to US_ASCII if no BOM.\n        // This fix preserves that aspect but points it out as a potential area for improvement\n        // for broader Unicode compatibility.\n        Charset encoding = StandardCharsets.US_ASCII; \n\n\n        // Check for a UTF-8 BOM at the beginning (EF BB BF)\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            encoding = StandardCharsets.UTF_8;\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n        // If no BOM is found, 'encoding' remains the initial default (US_ASCII in this snippet).\n        // A more robust parser would use its configured _defaultEncoding here.\n        // else {\n        //    encoding = _defaultEncoding;\n        // }\n\n\n        String contentAsStr;\n        contentAsStr = new String(content, offset, bytesLen, encoding);\n\n        // Decide if we need to do special HTML processing.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        boolean hasHTML = false;\n        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n                LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n                } else {\n                    LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n                }\n                hasHTML = true;\n            }\n        }\n\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        // Assuming ParseState constructor can take exactUserAgentMatching\n        ParseState parseState = new ParseState(url, robotNames, exactUserAgentMatching);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            if (hasHTML) {\n                line = line.replaceAll(\"<[^>]+>\", \"\");\n            }\n\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            line = line.trim();\n            if (line.length() == 0) {\n                continue;\n            }\n\n            RobotToken token = tokenize(line);\n            switch (token.getDirective()) {\n                case USER_AGENT:\n                handleUserAgent(parseState, token);\n                    break;\n\n                case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, token); // Uses the modified version\n                    break;\n\n                case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, token); // Uses the modified version\n                    break;\n\n                case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, token);\n                    break;\n\n                case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, token);\n                    break;\n\n                case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, token);\n                    break;\n\n                case UNKNOWN:\n                // Assuming reportWarning is an instance method of this class\n                this.reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                case MISSING:\n                this.reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                default:\n                    break;\n            }\n        }\n\n        this._numWarningsDuringLastParse.set(parseState.getNumWarnings()); // Assuming getNumWarnings() getter\n        SimpleRobotRules result = parseState.getRobotRules();\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n            result.sortRules();\n            return result;\n        }\n    }\n",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}