{
    "generation": "import crawlercommons.robots.SimpleRobotRules.RobotRulesMode;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.UnsupportedEncodingException;\nimport java.net.URLDecoder;\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.StringTokenizer;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n// Assume necessary imports and surrounding class structure exist (e.g., fields like LOGGER, _numWarningsDuringLastParse, _maxCrawlDelay, patterns, handler methods, RobotToken, ParseState, etc.)\n// We only provide the fixed method as requested.\n\n    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        // Default to UTF-8 encoding per IETF draft standard (draft-koster-rep).\n        // This is the most common encoding for robots.txt and handles international characters.\n        Charset encoding = StandardCharsets.UTF_8;\n\n        // Check for UTF BOMs to override the default UTF-8.\n        // Check for a UTF-8 BOM at the beginning (EF BB BF). Note: BOM is discouraged in UTF-8, but we handle it.\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            // Encoding is already UTF-8, just adjust offset/length.\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n        // No BOM detected, proceed with the default (UTF-8).\n\n        String contentAsStr;\n        try {\n             // Decode using the detected or default (UTF-8) encoding.\n             // Malformed sequences according to the charset will be replaced by the default replacement character.\n            contentAsStr = new String(content, offset, bytesLen, encoding);\n        } catch (UnsupportedOperationException | IllegalArgumentException e) {\n            // Should not happen with standard charsets, but handle defensively.\n            LOGGER.warn(\"Could not decode robots.txt using encoding {} for url: {} - {}\", encoding, url, e.getMessage());\n            // Treat as empty/allow all if decoding fails completely.\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n\n        // Decide if we need to do special HTML processing based on content type.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        // If it looks like it contains HTML tags (either by content type or basic pattern matching),\n        // check if it also contains a \"user-agent\" line. If not, assume it's an error page\n        // and not a real robots.txt file.\n        boolean hasHTML = false;\n        // Note: SIMPLE_HTML_PATTERN is a basic check and might not be exhaustive.\n        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n            // USER_AGENT_PATTERN check should be case-insensitive.\n            Matcher userAgentMatcher = USER_AGENT_PATTERN.matcher(contentAsStr);\n            if (!userAgentMatcher.find()) {\n                // No User-agent found in potential HTML content.\n                LOGGER.trace(\"Found non-robots.txt HTML file (no User-agent): {}\", url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // Found HTML, but also a User-agent line, so proceed with parsing but enable HTML stripping.\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: {}\", url);\n                } else {\n                    LOGGER.debug(\"Found HTML tags in robots.txt file: {}\", url);\n                }\n                hasHTML = true;\n            }\n        }\n\n        // Tokenize the content into logical lines based on standard line breaks.\n        // StringTokenizer skips empty tokens which works well for multiple line breaks.\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        // Initialize the state for parsing, passing necessary parameters.\n        ParseState parseState = new ParseState(url, robotNames, exactUserAgentMatching);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            // If HTML was detected, attempt to strip basic tags before further processing.\n            if (hasHTML) {\n                // This regex removes content between < and >. It's basic and might fail on complex/broken HTML.\n                line = line.replaceAll(\"<[^>]+>\", \"\");\n            }\n\n            // Remove comments starting with '#'.\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            // Trim leading/trailing whitespace.\n            line = line.trim();\n\n            // Skip lines that are now empty (e.g., were comments or whitespace only).\n            if (line.isEmpty()) {\n                continue;\n            }\n\n            // Tokenize the line into a directive and its value (e.g., \"Disallow: /path\").\n            // Assumes tokenize handles case-insensitivity of the directive and extracts the value.\n            RobotToken token = tokenize(line);\n\n            // Process the token based on the directive.\n            // Rules like Allow/Disallow apply to the *current* User-agent group being processed.\n            // Sitemaps are global. Unknown/invalid lines are ignored per spec.\n            switch (token.getDirective()) {\n                case USER_AGENT:\n                    // handleUserAgent should update the current agent context within parseState.\n                    handleUserAgent(parseState, token);\n                    break;\n\n                case DISALLOW:\n                    // handleDisallow should add the rule (after percent-decoding the path)\n                    // to the current agent(s) being tracked by parseState.\n                    handleDisallow(parseState, token);\n                    break;\n\n                case ALLOW:\n                    // handleAllow should add the rule (after percent-decoding the path)\n                    // to the current agent(s) being tracked by parseState.\n                    handleAllow(parseState, token);\n                    break;\n\n                case CRAWL_DELAY:\n                    // handleCrawlDelay should set the delay for the current agent(s) in parseState.\n                    handleCrawlDelay(parseState, token);\n                    break;\n\n                case SITEMAP:\n                    // handleSitemap should add the sitemap URL (global context).\n                    handleSitemap(parseState, token);\n                    break;\n\n                case HTTP:\n                    // This directive is non-standard. The original code handled it; maintain behavior or log/ignore.\n                    handleHttp(parseState, token);\n                    break;\n\n                case UNKNOWN:\n                    // Log unknown directives. The specification requires ignoring them.\n                    reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                    break;\n\n                case MISSING:\n                    // Line doesn't conform to \"directive: value\". The specification requires ignoring it.\n                    reportWarning(parseState, \"Ignoring line with unexpected format in robots.txt file (size {}): {}\", content.length, line);\n                    break;\n\n                default:\n                    // Ignore any other directives encountered (e.g., Host, Noindex if not explicitly handled).\n                    break;\n            }\n        }\n\n        // Finished processing all lines. Store the number of warnings encountered.\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        // Retrieve the compiled rules from the parse state.\n        SimpleRobotRules result = parseState.getRobotRules();\n\n        // Enforce a maximum crawl delay. If the parsed delay exceeds the limit,\n        // treat it as if the site disallows everything (for the matched agent).\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            // Note: Returning a generic ALLOW_NONE might discard sitemap information.\n            // A more nuanced approach could modify the existing rules, but we follow the original logic.\n            LOGGER.debug(\"Crawl delay ({}) for {} exceeds max ({}) - disallowing all URLs: {}\",\n                         result.getCrawlDelay(), parseState.activeAgent(), _maxCrawlDelay, url); // Assumes parseState.activeAgent() gives relevant agent name\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n            // Sort the rules (Allow/Disallow) for potentially faster matching.\n            result.sortRules();\n            return result;\n        }\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}