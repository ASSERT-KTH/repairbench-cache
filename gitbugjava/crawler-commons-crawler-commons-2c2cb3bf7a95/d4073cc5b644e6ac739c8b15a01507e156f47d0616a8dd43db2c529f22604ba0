{
    "generation": "    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        // Per Google spec, default encoding is UTF-8\n        // https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#file-format\n        Charset encoding = StandardCharsets.UTF_8;\n\n        // Check for a UTF-8 BOM at the beginning (EF BB BF)\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            // Encoding is already UTF-8\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n        // Although not standard, check for UTF-32LE BOM (FF FE 00 00)\n        else if ((bytesLen >= 4) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE) && (content[2] == (byte) 0x00) && (content[3] == (byte) 0x00)) {\n            offset = 4;\n            bytesLen -= 4;\n            // Java doesn't have a standard Charset name for UTF_32LE?\n            // Use available name if possible, otherwise stick to UTF-8 as fallback\n            try {\n                encoding = Charset.forName(\"UTF-32LE\");\n            } catch (UnsupportedCharsetException e) {\n                LOGGER.warn(\"UTF-32LE BOM detected, but Charset is not supported. Falling back to UTF-8 for URL: {}\", url);\n                encoding = StandardCharsets.UTF_8; // Fallback, though likely incorrect.\n                // Reset offset/bytesLen as we can't decode it properly without the charset.\n                offset = 0;\n                bytesLen = content.length;\n            }\n        }\n        // Although not standard, check for UTF-32BE BOM (00 00 FE FF)\n        else if ((bytesLen >= 4) && (content[0] == (byte) 0x00) && (content[1] == (byte) 0x00) && (content[2] == (byte) 0xFE) && (content[3] == (byte) 0xFF)) {\n            offset = 4;\n            bytesLen -= 4;\n            try {\n                encoding = Charset.forName(\"UTF-32BE\");\n            } catch (UnsupportedCharsetException e) {\n                 LOGGER.warn(\"UTF-32BE BOM detected, but Charset is not supported. Falling back to UTF-8 for URL: {}\", url);\n                 encoding = StandardCharsets.UTF_8; // Fallback\n                 offset = 0;\n                 bytesLen = content.length;\n            }\n        }\n\n\n        String contentAsStr;\n        try {\n            // MalformedInputException or UnmappableCharacterException can occur\n            // If decoding fails, treat as empty as we can't parse reliably.\n            contentAsStr = new String(content, offset, bytesLen, encoding);\n        } catch (Exception e) {\n             LOGGER.warn(\"Failed to decode robots.txt using charset {}. Treating as empty for URL: {}\", encoding, url, e);\n             return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n\n        // Decide if we need to do special HTML processing.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        // If it looks like it contains HTML, but doesn't have a user agent\n        // field, then\n        // assume somebody messed up and returned back to us a random HTML page\n        // instead\n        // of a robots.txt file.\n        boolean hasHTML = false;\n        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n                LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // We'll try to strip out HTML tags below.\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n                } else {\n                    LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n                }\n\n                hasHTML = true;\n            }\n        }\n\n        // Break on anything that might be used as a line ending. Since\n        // tokenizer doesn't return empty tokens, a \\r\\n sequence still\n        // works since it looks like an empty string between the \\r and \\n.\n        // Using split with a regex is generally safer for handling various line endings correctly.\n        // StringTokenizer might miss empty lines depending on the sequence of delimiters.\n        // Using split(\"[\\n\\r\\u0085\\u2028\\u2029]+\") handles multiple consecutive breaks better.\n        String[] lines = contentAsStr.split(\"[\\n\\r\\u0085\\u2028\\u2029]+\");\n        ParseState parseState = new ParseState(url, robotNames, exactUserAgentMatching); // Pass exactUserAgentMatching to ParseState constructor\n\n        for (String line : lines) {\n             // Get rid of HTML markup, in case some brain-dead webmaster has\n             // created an HTML page for robots.txt.\n             if (hasHTML) {\n                 // Basic removal, might not cover all edge cases like tags spanning lines (though split helps).\n                 line = line.replaceAll(\"<[^>]+>\", \"\");\n             }\n\n             // trim out comments and whitespace\n             int hashPos = line.indexOf(\"#\");\n             if (hashPos >= 0) {\n                 line = line.substring(0, hashPos);\n             }\n\n             line = line.trim();\n             if (line.length() == 0) {\n                 continue;\n             }\n\n             RobotToken token = tokenize(line);\n             switch (token.getDirective()) {\n                 case USER_AGENT:\n                 handleUserAgent(parseState, token);\n                     break;\n\n                 case DISALLOW:\n                 parseState.setFinishedAgentFields(true);\n                 handleDisallow(parseState, token);\n                     break;\n\n                 case ALLOW:\n                 parseState.setFinishedAgentFields(true);\n                 handleAllow(parseState, token);\n                     break;\n\n                 case CRAWL_DELAY:\n                 parseState.setFinishedAgentFields(true);\n                 handleCrawlDelay(parseState, token);\n                     break;\n\n                 case SITEMAP:\n                 // Sitemaps are not agent-specific, but processing them after agent fields seems reasonable.\n                 // Google treats them as independent of any agent block.\n                 // parseState.setFinishedAgentFields(true); // This might be incorrect depending on desired grouping behavior. Let ParseState handle it.\n                 handleSitemap(parseState, token);\n                     break;\n\n                 case HTTP:\n                 parseState.setFinishedAgentFields(true);\n                 handleHttp(parseState, token);\n                     break;\n\n                 case UNKNOWN:\n                 // Per Google Spec, ignore unknown directives. Log as trace or debug instead of warning.\n                 LOGGER.trace(\"Unknown directive in robots.txt file: {} in {}\", line, url);\n                 // parseState.setFinishedAgentFields(true); // Unknown directives shouldn't affect agent grouping.\n                     break;\n\n                 case MISSING:\n                 // This typically means a line couldn't be parsed into field:value. Ignore per Google spec.\n                 LOGGER.trace(\"Ignoring unparseable line in robots.txt file (size {}): {} in {}\", content.length, line, url);\n                 // parseState.setFinishedAgentFields(true); // Unparseable lines shouldn't affect agent grouping.\n                     break;\n\n                 default:\n                     // Includes HOST, NOINDEX, REQUEST_RATE, VISIT_TIME etc. which might be handled specifically or ignored.\n                     // Currently ignored. Setting finishedAgentFields depends on the directive.\n                     // For now, assume they don't impact agent grouping unless handled specifically.\n                     // TODO KKr - handle no-index\n                     // TODO KKr - handle request-rate and visit-time\n                     break;\n             }\n         }\n\n\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        SimpleRobotRules result = parseState.getRobotRules();\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            // Some evil sites use a value like 3600 (seconds) for the crawl\n            // delay, which would cause lots of problems for us.\n            LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n            // Create rules disallowing everything but retain sitemaps and other global rules.\n            SimpleRobotRules restrictedRules = new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n            restrictedRules.copyGlobals(result); // Copy sitemaps etc.\n            return restrictedRules;\n        } else {\n            result.sortRules();\n            return result;\n        }\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}