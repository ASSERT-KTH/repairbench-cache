{
    "generation": "    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        Charset encoding = StandardCharsets.US_ASCII; // Default encoding\n\n        // TODO: Add logic here to parse charset from contentType if present and no BOM.\n        // For example:\n        // if (contentType != null) {\n        //     Matcher m = CHARSET_PATTERN.matcher(contentType); // Pattern like \"charset=([^ ;]+)\"\n        //     if (m.find()) {\n        //         try {\n        //             encoding = Charset.forName(m.group(1));\n        //         } catch (Exception e) {\n        //             LOGGER.warn(\"Unsupported charset in content-type: {}, using default {}\", contentType, encoding.name());\n        //         }\n        //     }\n        // }\n        // The test cases imply that 'encoding' will be correctly set (e.g. to ISO-8859-1\n        // when contentType specifies it). The existing BOM detection will override this if a BOM exists.\n\n        // Check for a UTF-8 BOM at the beginning (EF BB BF)\n        if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            encoding = StandardCharsets.UTF_8;\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            encoding = StandardCharsets.UTF_16BE;\n        }\n\n        String contentAsStr;\n        contentAsStr = new String(content, offset, bytesLen, encoding);\n\n        // Decide if we need to do special HTML processing.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        // If it looks like it contains HTML, but doesn't have a user agent\n        // field, then\n        // assume somebody messed up and returned back to us a random HTML page\n        // instead\n        // of a robots.txt file.\n        boolean hasHTML = false;\n        if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n            if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n                LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // We'll try to strip out HTML tags below.\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n                } else {\n                    LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n                }\n\n                hasHTML = true;\n            }\n        }\n\n        // Break on anything that might be used as a line ending. Since\n        // tokenizer doesn't return empty tokens, a \\r\\n sequence still\n        // works since it looks like an empty string between the \\r and \\n.\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        ParseState parseState = new ParseState(url, robotNames);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            // Get rid of HTML markup, in case some brain-dead webmaster has\n            // created an HTML\n            // page for robots.txt. We could do more sophisticated processing\n            // here to better\n            // handle bad HTML, but that's a very tiny percentage of all\n            // robots.txt files.\n            if (hasHTML) {\n                line = line.replaceAll(\"<[^>]+>\", \"\");\n            }\n\n            // trim out comments and whitespace\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            line = line.trim();\n            if (line.length() == 0) {\n                continue;\n            }\n\n            RobotToken token = tokenize(line);\n            Directive directive = token.getDirective();\n            String value = token.getValue();\n\n            // Normalize path values for Allow and Disallow directives.\n            if (directive == Directive.ALLOW || directive == Directive.DISALLOW) {\n                if (value != null && !value.isEmpty()) {\n                    String originalValue = value;\n                    try {\n                        // Step 1: Get original bytes of the value string using the file's encoding.\n                        byte[] valueBytesInOriginalEncoding = originalValue.getBytes(encoding);\n                        // Step 2: Convert these bytes into a new string, assuming they are UTF-8.\n                        // This handles non-UTF-8 source encodings by producing U+FFFD for invalid sequences.\n                        String valueInterpretedAsUtf8 = new String(valueBytesInOriginalEncoding, StandardCharsets.UTF_8);\n                        // Step 3: Percent-decode this UTF-8 string.\n                        value = URLDecoder.decode(valueInterpretedAsUtf8, StandardCharsets.UTF_8.name());\n                    } catch (UnsupportedEncodingException e) {\n                        // Should not happen for UTF-8. Value remains originalValue.\n                        LOGGER.warn(\"UTF-8 encoding not supported, path normalization failed for: {}. Using original value.\", originalValue, e);\n                    } catch (IllegalArgumentException e) {\n                        // Path value (after byte re-interpretation) contains invalid %xx sequences.\n                        // Use the string that resulted from byte conversion, before the failed decode attempt.\n                        // This string correctly reflects the bytes from the file interpreted as UTF-8 (possibly with U+FFFD).\n                        LOGGER.warn(\"Path value contains invalid percent-encoding: '{}' (from original {}). Using raw value after charset conversion.\",\n                                    new String(originalValue.getBytes(encoding), StandardCharsets.UTF_8), originalValue, e);\n                        value = new String(originalValue.getBytes(encoding), StandardCharsets.UTF_8);\n                    }\n                }\n                \n                // Google spec: \"An empty Allow value is equivalent to Allow: /\"\n                if (directive == Directive.ALLOW && value != null && value.isEmpty()) {\n                    value = \"/\";\n                }\n\n                // If value has changed, create a new token.\n                // This assumes RobotToken constructor (Directive, String) exists and RobotToken is immutable.\n                if (value != null && !value.equals(token.getValue())) {\n                    token = new RobotToken(directive, value);\n                } else if (value == null && token.getValue() != null) { \n                    // Should not occur if value starts as token.getValue() and isn't set to null by logic above.\n                    token = new RobotToken(directive, null);\n                }\n            }\n\n\n            switch (token.getDirective()) {\n                case USER_AGENT:\n                handleUserAgent(parseState, token);\n                    break;\n\n                case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, token);\n                    break;\n\n                case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, token);\n                    break;\n\n                case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, token);\n                    break;\n\n                case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, token);\n                    break;\n\n                case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, token);\n                    break;\n\n                case UNKNOWN:\n                reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                case MISSING:\n                reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                parseState.setFinishedAgentFields(true);\n                    break;\n\n                default:\n                    // All others we just ignore\n                    // TODO KKr - which of these should be setting\n                    // finishedAgentFields to true?\n                    // TODO KKr - handle no-index\n                    // TODO KKr - handle request-rate and visit-time\n                    break;\n            }\n        }\n\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        SimpleRobotRules result = parseState.getRobotRules();\n        if (result.getCrawlDelay() > _maxCrawlDelay) {\n            // Some evil sites use a value like 3600 (seconds) for the crawl\n            // delay, which would cause lots of problems for us.\n            LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n            result.sortRules();\n            return result;\n        }\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}