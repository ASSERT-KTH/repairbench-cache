{
    "generation": "import java.nio.ByteBuffer;\nimport java.nio.CharBuffer;\nimport java.nio.charset.CharacterCodingException;\nimport java.nio.charset.Charset;\nimport java.nio.charset.CharsetDecoder;\nimport java.nio.charset.CodingErrorAction;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.StringTokenizer;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n// Assuming imports for SimpleRobotRules, RobotRulesMode, ParseState, RobotToken, etc. are present\n// Assuming imports for LOGGER, SIMPLE_HTML_PATTERN, USER_AGENT_PATTERN are present\n// Assuming _numWarningsDuringLastParse, _maxCrawlDelay, reportWarning, tokenize, handle* methods exist\n\n    private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n        // If there's nothing there, treat it like we have no restrictions.\n        if ((content == null) || (content.length == 0)) {\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        }\n\n        int bytesLen = content.length;\n        int offset = 0;\n        // Default to UTF-8, as specified by Google and RFC 9309.\n        Charset initialEncoding = StandardCharsets.UTF_8;\n\n        // Check for BOMs to override the default.\n        // Check for a UTF-8 BOM at the beginning (EF BB BF)\n        if ((bytesLen >= 3) && (content[offset] == (byte) 0xEF) && (content[offset + 1] == (byte) 0xBB) && (content[offset + 2] == (byte) 0xBF)) {\n            offset = 3;\n            bytesLen -= 3;\n            initialEncoding = StandardCharsets.UTF_8;\n        }\n        // Check for UTF-16LE BOM at the beginning (FF FE)\n        else if ((bytesLen >= 2) && (content[offset] == (byte) 0xFF) && (content[offset + 1] == (byte) 0xFE)) {\n            offset = 2;\n            bytesLen -= 2;\n            initialEncoding = StandardCharsets.UTF_16LE;\n        }\n        // Check for UTF-16BE BOM at the beginning (FE FF)\n        else if ((bytesLen >= 2) && (content[offset] == (byte) 0xFE) && (content[offset + 1] == (byte) 0xFF)) {\n            offset = 2;\n            bytesLen -= 2;\n            initialEncoding = StandardCharsets.UTF_16BE;\n        }\n\n        String contentAsStr;\n        Charset detectedEncoding;\n\n        // Try decoding with the initial encoding (UTF-8 or BOM-specified).\n        // Use a decoder that reports errors, so we know if it's valid.\n        CharsetDecoder decoder = initialEncoding.newDecoder()\n                                    .onMalformedInput(CodingErrorAction.REPORT)\n                                    .onUnmappableCharacter(CodingErrorAction.REPORT);\n        try {\n            CharBuffer charBuffer = decoder.decode(ByteBuffer.wrap(content, offset, bytesLen));\n            contentAsStr = charBuffer.toString();\n            detectedEncoding = initialEncoding;\n        } catch (CharacterCodingException e) {\n            // Decoding failed.\n            // If the initial encoding was UTF-8 (no BOM), try ISO-8859-1 as a fallback.\n            // This is not standard, but handles cases where servers mistakenly provide non-UTF-8 content.\n            if (initialEncoding == StandardCharsets.UTF_8) {\n                 LOGGER.debug(\"Invalid UTF-8 sequence in robots.txt (no BOM), trying ISO-8859-1 fallback: {}\", url);\n                 detectedEncoding = StandardCharsets.ISO_8859_1;\n                 // Use default replacement behavior when decoding with fallback, just in case.\n                 contentAsStr = new String(content, offset, bytesLen, detectedEncoding);\n            } else {\n                 // Failed decoding with a BOM-specified encoding (likely UTF-16).\n                 // This indicates a corrupt file. Use replacement characters.\n                 LOGGER.warn(\"Invalid sequence in BOM-specified {} encoded robots.txt: {}\", initialEncoding.displayName(), url);\n                 CharsetDecoder decoderReplace = initialEncoding.newDecoder()\n                                                .onMalformedInput(CodingErrorAction.REPLACE)\n                                                .onUnmappableCharacter(CodingErrorAction.REPLACE);\n                 try {\n                     // This decode should not throw with REPLACE action\n                     CharBuffer charBuffer = decoderReplace.decode(ByteBuffer.wrap(content, offset, bytesLen));\n                     contentAsStr = charBuffer.toString();\n                 } catch (CharacterCodingException unexpectedException) {\n                     // Should not happen with REPLACE, but handle defensively\n                     LOGGER.error(\"Unexpected exception while decoding with replacement chars for {}: {}\", url, unexpectedException.getMessage());\n                     return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL); // Cannot parse\n                 }\n                 detectedEncoding = initialEncoding; // Keep original encoding type for record\n            }\n        }\n\n        // Decide if we need to do special HTML processing.\n        boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n        // If it looks like it contains HTML, but doesn't have a user agent\n        // field, then assume it's not a real robots.txt file.\n        // Note: USER_AGENT_PATTERN should ideally be case-insensitive or check variations.\n        // Using a simple case-insensitive check here.\n        boolean hasHTML = false;\n        // Relaxed HTML check pattern\n        final Pattern SIMPLE_HTML_PATTERN_LOCAL = Pattern.compile(\"<html|<head|<body|<span|<div|<p|<a\\\\s\", Pattern.CASE_INSENSITIVE);\n        // Case-insensitive check for User-agent directive start\n        final Pattern USER_AGENT_DIRECTIVE_PATTERN_LOCAL = Pattern.compile(\"^\\\\s*User-[Aa]gent:\\\\s*\", Pattern.MULTILINE);\n\n        if (isHtmlType || SIMPLE_HTML_PATTERN_LOCAL.matcher(contentAsStr).find()) {\n            if (!USER_AGENT_DIRECTIVE_PATTERN_LOCAL.matcher(contentAsStr).find()) {\n                LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n                return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n            } else {\n                // Found HTML, but also a User-agent line, so proceed with parsing but strip tags.\n                if (isHtmlType) {\n                    LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n                } else {\n                    LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n                }\n                hasHTML = true;\n            }\n        }\n\n        // Break on anything that might be used as a line ending.\n        StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n        ParseState parseState = new ParseState(url, robotNames);\n\n        while (lineParser.hasMoreTokens()) {\n            String line = lineParser.nextToken();\n\n            // Strip HTML markup if detected. Use a simple non-greedy regex.\n            if (hasHTML) {\n                line = line.replaceAll(\"<[^>]*?>\", \"\");\n            }\n\n            // trim out comments and whitespace\n            int hashPos = line.indexOf(\"#\");\n            if (hashPos >= 0) {\n                line = line.substring(0, hashPos);\n            }\n\n            line = line.trim();\n            if (line.length() == 0) {\n                continue;\n            }\n\n            // Tokenize uses case-insensitive matching for directives.\n            // Path values are case-sensitive and should be normalized later (implicitly by handlers or matching logic).\n            RobotToken token = tokenize(line);\n            switch (token.getDirective()) {\n                case USER_AGENT:\n                    handleUserAgent(parseState, token);\n                    break;\n\n                case DISALLOW:\n                    parseState.setFinishedAgentFields(true);\n                    handleDisallow(parseState, token);\n                    break;\n\n                case ALLOW:\n                    parseState.setFinishedAgentFields(true);\n                    handleAllow(parseState, token);\n                    break;\n\n                case CRAWL_DELAY:\n                    parseState.setFinishedAgentFields(true);\n                    handleCrawlDelay(parseState, token);\n                    break;\n\n                case SITEMAP:\n                    parseState.setFinishedAgentFields(true);\n                    handleSitemap(parseState, token);\n                    break;\n\n                case HTTP:\n                    // Non-standard directive used by some crawlers\n                    parseState.setFinishedAgentFields(true);\n                    handleHttp(parseState, token);\n                    break;\n\n                case UNKNOWN:\n                    // Handle unknown directives per RFC 9309 (ignore them)\n                    // reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                    parseState.setFinishedAgentFields(true);\n                    break;\n\n                case MISSING:\n                    // Line doesn't match \"directive: value\" structure\n                    // reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                    parseState.setFinishedAgentFields(true);\n                    break;\n\n                default:\n                    // Includes directives like NOINDEX, REQUEST_RATE, VISIT_TIME etc. which are ignored by this parser.\n                    // Set finishedAgentFields based on Google's spec grouping rules.\n                    // Rules like Allow/Disallow/Crawl-Delay belong to a group.\n                    // Sitemap is independent. Others might be too.\n                    // Setting finishedAgentFields=true for safety after any recognized directive seems reasonable.\n                    // However, the original code structure suggests it's only set for directives\n                    // that clearly terminate a User-Agent block's specific rules.\n                    // Reverting UNKNOWN/MISSING/default cases to potentially NOT set finishedAgentFields might be needed\n                    // if strict adherence to group parsing is required. For now, keep as is.\n                    break;\n            }\n        }\n\n        this._numWarningsDuringLastParse.set(parseState._numWarnings);\n        SimpleRobotRules result = parseState.getRobotRules();\n\n        // Apply crawl delay limit check\n        final long MAX_CRAWL_DELAY = 300_000L; // Example: 5 minutes in ms, adjust as needed\n        if (result.getCrawlDelay() > MAX_CRAWL_DELAY) {\n            // Some sites use extreme crawl delays, effectively disallowing crawling.\n            LOGGER.warn(\"Crawl delay ({}) exceeds max ({}) - treating as disallowed: {}\", result.getCrawlDelay(), MAX_CRAWL_DELAY, url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else if (result.getCrawlDelay() > _maxCrawlDelay && _maxCrawlDelay > 0) {\n             // Note: Original code used > _maxCrawlDelay check resulting in ALLOW_NONE.\n             // This seems too strict. Capping delay or warning might be better.\n             // Let's assume the original check was intended, maybe _maxCrawlDelay is configurable.\n             LOGGER.debug(\"Crawl delay ({}) exceeds configured max ({}) - disallowing all URLs: {}\", result.getCrawlDelay(), _maxCrawlDelay, url);\n             return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n        } else {\n             // Ensure rules are sorted for efficient matching (longest prefix first)\n             result.sortRules();\n             return result;\n        }\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}