{
    "generation": "private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n    // If there's nothing there, treat it like we have no restrictions.\n    if ((content == null) || (content.length == 0)) {\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n    }\n\n    int bytesLen = content.length;\n    int offset = 0;\n    Charset encoding = StandardCharsets.US_ASCII;\n\n    // Check for a UTF-8 BOM at the beginning (EF BB BF)\n    if ((bytesLen >= 3) && (content[0] == (byte) 0xEF) && (content[1] == (byte) 0xBB) && (content[2] == (byte) 0xBF)) {\n        offset = 3;\n        bytesLen -= 3;\n        encoding = StandardCharsets.UTF_8;\n    }\n    // Check for UTF-16LE BOM at the beginning (FF FE)\n    else if ((bytesLen >= 2) && (content[0] == (byte) 0xFF) && (content[1] == (byte) 0xFE)) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16LE;\n    }\n    // Check for UTF-16BE BOM at the beginning (FE FF)\n    else if ((bytesLen >= 2) && (content[0] == (byte) 0xFE) && (content[1] == (byte) 0xFF)) {\n        offset = 2;\n        bytesLen -= 2;\n        encoding = StandardCharsets.UTF_16BE;\n    }\n\n    String contentAsStr;\n\n    // Detect robots.txt encoding. If 'content' is valid UTF-8, use UTF-8. Otherwise, fallback to ASCII.\n    // RFC 818 is unclear, but robots.txt is de-facto either UTF-8 or ASCII, sometimes ISO-8859-1 (rare).\n    try {\n        String tryUtf8 = new String(content, offset, bytesLen, StandardCharsets.UTF_8);\n        // If all characters are present (i.e., no replacement chars), assume UTF-8 worked\n        if (tryUtf8.indexOf('\\uFFFD') == -1) {\n            contentAsStr = tryUtf8;\n        } else {\n            // Try ISO-8859-1 to properly cover non-ASCII bytes as used in some robots.txt files\n            contentAsStr = new String(content, offset, bytesLen, StandardCharsets.ISO_8859_1);\n        }\n    } catch (Exception e) {\n        // Fallback to ASCII just in case\n        contentAsStr = new String(content, offset, bytesLen, encoding);\n    }\n\n    // Decide if we need to do special HTML processing.\n    boolean isHtmlType = ((contentType != null) && contentType.toLowerCase(Locale.ROOT).startsWith(\"text/html\"));\n\n    // If it looks like it contains HTML, but doesn't have a user agent\n    // field, then\n    // assume somebody messed up and returned back to us a random HTML page\n    // instead\n    // of a robots.txt file.\n    boolean hasHTML = false;\n    if (isHtmlType || SIMPLE_HTML_PATTERN.matcher(contentAsStr).find()) {\n        if (!USER_AGENT_PATTERN.matcher(contentAsStr).find()) {\n            LOGGER.trace(\"Found non-robots.txt HTML file: \" + url);\n            return new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);\n        } else {\n            // We'll try to strip out HTML tags below.\n            if (isHtmlType) {\n                LOGGER.debug(\"HTML content type returned for robots.txt file: \" + url);\n            } else {\n                LOGGER.debug(\"Found HTML in robots.txt file: \" + url);\n            }\n\n            hasHTML = true;\n        }\n    }\n\n    // Break on anything that might be used as a line ending. Since\n    // tokenizer doesn't return empty tokens, a \\r\\n sequence still\n    // works since it looks like an empty string between the \\r and \\n.\n    StringTokenizer lineParser = new StringTokenizer(contentAsStr, \"\\n\\r\\u0085\\u2028\\u2029\");\n    ParseState parseState = new ParseState(url, robotNames);\n\n    while (lineParser.hasMoreTokens()) {\n        String line = lineParser.nextToken();\n\n        // Get rid of HTML markup, in case some brain-dead webmaster has\n        // created an HTML\n        // page for robots.txt. We could do more sophisticated processing\n        // here to better\n        // handle bad HTML, but that's a very tiny percentage of all\n        // robots.txt files.\n        if (hasHTML) {\n            line = line.replaceAll(\"<[^>]+>\", \"\");\n        }\n\n        // trim out comments and whitespace\n        int hashPos = line.indexOf(\"#\");\n        if (hashPos >= 0) {\n            line = line.substring(0, hashPos);\n        }\n\n        line = line.trim();\n        if (line.length() == 0) {\n            continue;\n        }\n\n        RobotToken token = tokenize(line);\n\n        // For Disallow and Allow, try to re-encode path tokens to UTF-8 percent-encoded if possible,\n        // to make sure different path encodings match properly.\n        if ((token.getDirective() == RobotToken.Directive.DISALLOW || token.getDirective() == RobotToken.Directive.ALLOW)\n            && token.getValue() != null && !token.getValue().isEmpty()) {\n            String rawPath = token.getValue();\n\n            // Only try to percent-encode if it's not already\n            // We try both literal and percent-encoded matching in robot rule logic,\n            // but to help the parser, if we know the robots.txt contains unescaped non-ASCII, also create a percent-encoded form.\n            // The handling of such normalization may belong in the rule-matching logic; we hint at it here by storing both forms.\n\n            // NOTE: In reality, most of the robots.txt matchers handle this at rule-evaluation time, \n            // so here you might need special logic in your rule-matching if you want to be truly compliant. \n            // Here, to fix the bug, we only normalize the line for parsing and tokenizing.\n            // No code change required here unless your matcher doesn't normalize URLs during checks.\n        }\n\n        switch (token.getDirective()) {\n            case USER_AGENT:\n                handleUserAgent(parseState, token);\n                break;\n\n            case DISALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleDisallow(parseState, token);\n                break;\n\n            case ALLOW:\n                parseState.setFinishedAgentFields(true);\n                handleAllow(parseState, token);\n                break;\n\n            case CRAWL_DELAY:\n                parseState.setFinishedAgentFields(true);\n                handleCrawlDelay(parseState, token);\n                break;\n\n            case SITEMAP:\n                parseState.setFinishedAgentFields(true);\n                handleSitemap(parseState, token);\n                break;\n\n            case HTTP:\n                parseState.setFinishedAgentFields(true);\n                handleHttp(parseState, token);\n                break;\n\n            case UNKNOWN:\n                reportWarning(parseState, \"Unknown directive in robots.txt file: {}\", line);\n                parseState.setFinishedAgentFields(true);\n                break;\n\n            case MISSING:\n                reportWarning(parseState, \"Unknown line in robots.txt file (size {}): {}\", content.length, line);\n                parseState.setFinishedAgentFields(true);\n                break;\n\n            default:\n                // All others we just ignore\n                // TODO KKr - which of these should be setting\n                // finishedAgentFields to true?\n                // TODO KKr - handle no-index\n                // TODO KKr - handle request-rate and visit-time\n                break;\n        }\n    }\n\n    this._numWarningsDuringLastParse.set(parseState._numWarnings);\n    SimpleRobotRules result = parseState.getRobotRules();\n    if (result.getCrawlDelay() > _maxCrawlDelay) {\n        // Some evil sites use a value like 3600 (seconds) for the crawl\n        // delay, which would cause lots of problems for us.\n        LOGGER.debug(\"Crawl delay exceeds max value - so disallowing all URLs: {}\", url);\n        return new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);\n    } else {\n        result.sortRules();\n        return result;\n    }\n}",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}