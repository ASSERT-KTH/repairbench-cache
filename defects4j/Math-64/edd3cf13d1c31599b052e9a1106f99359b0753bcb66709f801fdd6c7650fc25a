{
    "generation": "    /** {@inheritDoc} */\n    @Override\n    protected VectorialPointValuePair doOptimize()\n        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n\n        // arrays shared with the other private methods\n        solvedCols  = Math.min(rows, cols);\n        diagR       = new double[cols];\n        jacNorm     = new double[cols];\n        beta        = new double[cols];\n        permutation = new int[cols];\n        lmDir       = new double[cols];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[cols];\n        double[] oldX    = new double[cols];\n        double[] oldRes  = new double[rows];\n        double[] work1   = new double[cols];\n        double[] work2   = new double[cols];\n        double[] work3   = new double[cols];\n\n        // evaluate the function at the starting point and calculate its norm\n        updateResidualsAndCost(); // updates point, objective, cost, residuals\n\n        // outer loop\n        lmPar = 0;\n        boolean firstIteration = true;\n        // Use clones to avoid side effects modification of previous VPV Pair state\n        VectorialPointValuePair current = new VectorialPointValuePair(point.clone(), objective.clone());\n        while (true) {\n            incrementIterationsCounter();\n\n            // compute the Q.R. decomposition of the jacobian matrix\n            VectorialPointValuePair previous = current; // state from previous iteration\n            updateJacobian(); // computes jacobian matrix\n            qrDecomposition(); // computes QR decomposition, stores R in jacobian upper triangle\n\n            // compute Q^T.res\n            qTy(residuals); // residuals now holds Q^T * f(x)\n\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements explicitly\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                jacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < cols; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * point[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk; // diag is the scaling matrix D\n                }\n                xNorm = Math.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector f(x) and jacobian columns J_j\n            // using the QR decomposition: check max | (J^T f)_j | / (|| J_j || * || f || )\n            // (J^T f)_j = (P R^T Q^T f)_j. Let y = Q^T f (in residuals). (R^T y)_j = sum_{i=0..j} R_ij y_i\n            double maxCosine = 0;\n            if (cost != 0) { // Check cost != 0 to avoid division by zero\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj]; // || J_pj ||\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                             // Ensure index i is within bounds for residuals array\n                             if (i < rows) {\n                                sum += jacobian[i][pj] * residuals[i]; // R_ij * y_i\n                             }\n                        }\n                        maxCosine = Math.max(maxCosine, Math.abs(sum) / (s * cost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // convergence has been reached (gradient is orthogonal to columns)\n                // Return current point (from previous iteration), not previous!\n                return current;\n            }\n\n            // rescale D using the max value between initial and current jacobian column norms\n            for (int j = 0; j < cols; ++j) {\n                diag[j] = Math.max(diag[j], jacNorm[j]);\n            }\n\n            // inner loop (finding the LM step)\n            for (double ratio = 0; ratio < 1.0e-4;) { // Loop until acceptable step found (ratio >= 1e-4)\n\n                // save the state (current point and cost)\n                System.arraycopy(point, 0, oldX, 0, cols);\n                double previousCost = cost;\n                // Swap residual arrays, oldRes now holds Q^T*f_old\n                double[] tmpVec = residuals;\n                residuals = oldRes;\n                oldRes    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter (lmPar) and step (-lmDir)\n                // Solves (J^T J + lmPar D^T D) p = -J^T f_old using QR\n                // Returns lmDir = -p\n                determineLMParameter(oldRes, delta, diag, work1, work2, work3);\n\n                // compute the step p and the norm of the scaled step D*p\n                double lmNorm = 0;\n                // Negate lmDir to get the step p\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj]; // lmDir now holds p\n                }\n\n                // Calculate the new candidate point x_new = x_old + p, only for solved columns\n                // And calculate lmNorm = || D*p ||\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    point[pj] = oldX[pj] + lmDir[pj]; // Update point component\n                    double s = diag[pj] * lmDir[pj];  // D_jj * p_j\n                    lmNorm  += s * s;\n                }\n                // If solvedCols < cols, the remaining point components are unchanged.\n                // This assumes determineLMParameter correctly sets lmDir[pj] = 0 for non-solved cols,\n                // or that the optimization subspace is sufficient.\n\n                lmNorm = Math.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = Math.min(delta, lmNorm);\n                }\n\n                // evaluate the function at the trial point x + p\n                // This updates point, objective, cost=||f_new||, residuals=f_new\n                updateResidualsAndCost();\n                // Create a potential new state VPV pair\n                VectorialPointValuePair potential = new VectorialPointValuePair(point.clone(), objective.clone());\n\n                // compute the scaled actual reduction: Red_act = (||f_old||^2 - ||f_new||^2) / ||f_old||^2\n                double actRed = -1.0;\n                 // Avoid division by zero if previousCost is zero\n                if (previousCost > 1e-16) { // Check against small number instead of 0.1*cost\n                    double r = cost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction:\n                // Red_pred = (||f_old||^2 - || J*p + f_old ||^2) / ||f_old||^2\n                // Using LM relation: Red_pred = (p^T J^T J p + 2 p^T J^T f_old) / ||f_old||^2 ? No.\n                // PRED = - p^T J^T f_old + lmPar || D p ||^2\n                // Scaled PRED = (- p^T J^T f_old + lmPar || D p ||^2) / || f_old ||^2\n                // Using LM equation: p^T J^T J p + lmPar || D p ||^2 = - p^T J^T f_old\n                // Scaled PRED = (p^T J^T J p + 2 * lmPar || D p ||^2) / || f_old ||^2\n                // Scaled PRED = (|| R p_perm ||^2 + 2 * lmPar || D p_perm ||^2) / || Q^T f_old ||^2\n\n                // Calculate work1 = R * p_perm where p_perm = P^T p (p is in lmDir)\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj]; // p_perm[j]\n                    // Initialize work1[j] needed for accumulation below? No, work1[i] is target.\n                    // Ensure work1 is properly sized and initialized if needed before loop.\n                    // Assuming work1 is initialized to 0 somewhere (e.g., declaration or previous usage).\n                    // The calculation below accumulates into work1[i] based on columns j.\n                    // Needs work1[0..solvedCols-1] to be zero before the outer loop starts. Let's zero it here.\n                    if (j < work1.length) work1[j] = 0; // Zero out relevant part of work1\n                }\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj]; // p_perm[j]\n                    for (int i = 0; i <= j; ++i) {\n                         // Ensure index i is within bounds for jacobian rows and work1\n                         if (i < rows && i < work1.length) {\n                            work1[i] += jacobian[i][pj] * dirJ; // work1[i] += R_ij * p_perm[j]\n                         }\n                    }\n                }\n                // After loops, work1[i] = sum_{j=i..solvedCols-1} R_ij * p_perm[j] = (R * p_perm)_i\n\n                double coeff1 = 0; // || R p_perm ||^2\n                for (int j = 0; j < solvedCols; ++j) {\n                     if (j < work1.length) { // Ensure index j is within bounds for work1\n                        coeff1 += work1[j] * work1[j];\n                     }\n                }\n                double pc2 = previousCost * previousCost; // || f_old ||^2\n                if (pc2 < 1e-16) { // Handle potential division by zero\n                    pc2 = 1e-16;\n                }\n\n                coeff1 = coeff1 / pc2; // Scaled || R p_perm ||^2\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2; // Scaled lmPar * || D p ||^2\n                double preRed = coeff1 + 2 * coeff2; // Scaled predicted reduction\n\n                // compute the scaled directional derivative: dirDer = (p^T J^T f_old) / ||f_old||^2\n                // Using LM relation: dirDer = -(|| R p_perm ||^2 + lmPar || D p_perm ||^2) / ||f_old||^2\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound delta and LM parameter lmPar based on ratio\n                if (ratio <= 0.25) {\n                    // Bad step. Increase damping, shrink trust region.\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * cost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        // *** THE FIX: Use lmNorm instead of 10.0 * lmNorm ***\n                        delta = tmp * Math.min(delta, lmNorm);\n                        lmPar /= tmp; // Increase lmPar (damping)\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    // Good step. Decrease damping, expand trust region.\n                    delta = 2 * lmNorm; // MINPACK/Mor\u00e9 update rule for delta expansion\n                    lmPar *= 0.5; // Decrease lmPar (damping)\n                }\n                // If 0.25 < ratio < 0.75, delta and lmPar remain unchanged.\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, accept the new point\n                    firstIteration = false;\n                    // Update xNorm using the accepted point\n                    xNorm = 0;\n                    for (int k = 0; k < cols; ++k) {\n                        double xK = diag[k] * point[k];\n                        xNorm    += xK * xK;\n                    }\n                    xNorm = Math.sqrt(xNorm);\n\n                    // Update the current state VPV Pair\n                    current = potential;\n\n                    // tests for convergence using the accepted state\n                    if (checker != null) {\n                        if (checker.converged(getIterations(), previous, current)) {\n                            return current;\n                        }\n                    } else { // Default convergence criteria\n                        // Check based on the quality of the step just taken\n                        if (((Math.abs(actRed) <= costRelativeTolerance) &&\n                             (preRed <= costRelativeTolerance) &&\n                             (ratio <= 2.0)) ||\n                            (delta <= parRelativeTolerance * xNorm)) { // Check step size vs parameter norm\n                           return current;\n                        }\n                    }\n                } else {\n                    // failed iteration, reset point and cost to previous state\n                    cost = previousCost;\n                    System.arraycopy(oldX, 0, point, 0, cols); // Restore point from saved state\n                    // Restore residuals arrays\n                    tmpVec    = residuals;\n                    residuals = oldRes;\n                    oldRes    = tmpVec;\n                    // Note: current remains 'previous' as the potential state was not accepted.\n                }\n\n                // Check termination criteria (stringent tolerances)\n                // These checks should happen regardless of step success/failure\n                if ((Math.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                            costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                            parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    // Orthogonality check is done at the start of the outer loop. Checking here seems redundant\n                    // unless maxCosine was updated within the inner loop, which it isn't.\n                    // Keep it as a potential safeguard? Or remove it? Let's keep it as in original.\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                            orthoTolerance);\n                }\n            } // End inner loop (finding LM step)\n        } // End outer loop (iterations)\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": true,
    "test": false
}