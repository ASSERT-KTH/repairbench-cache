{
    "generation": "    /** {@inheritDoc} */\n    @Override\n    protected VectorialPointValuePair doOptimize()\n        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n\n        // arrays shared with the other private methods\n        solvedCols  = Math.min(rows, cols);\n        diagR       = new double[cols];\n        jacNorm     = new double[cols];\n        beta        = new double[cols];\n        permutation = new int[cols];\n        lmDir       = new double[cols];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[cols];\n        double[] oldX    = new double[cols];\n        double[] oldRes  = new double[rows];\n        double[] work1   = new double[cols];\n        double[] work2   = new double[cols];\n        double[] work3   = new double[cols];\n\n        // evaluate the function at the starting point and calculate its norm\n        updateResidualsAndCost(); // Computes cost = sqrt(sum w_i r_i^2), residuals = target - f(point)\n\n        // outer loop\n        lmPar = 0;\n        boolean firstIteration = true;\n        // The objective value passed to the pair is the residual vector (target - f(point))\n        // This is consistent with computeObjectiveValue implementation.\n        VectorialPointValuePair current = new VectorialPointValuePair(point, objective);\n        while (true) {\n            incrementIterationsCounter();\n\n            // compute the Q.R. decomposition of the jacobian matrix\n            VectorialPointValuePair previous = current;\n            updateJacobian(); // Computes Jacobian J = -df/dp (or df/dp depending on convention)\n            qrDecomposition(); // Computes Q, R, permutation such that J*P = Q*R\n\n            // compute Qt.res\n            // residuals contains (target - f(point)) from start of iteration\n            qTy(residuals); // residuals now contains g = Q^T * (target - f(point))\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                jacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < cols; ++k) {\n                    double dk = jacNorm[k]; // Norm of k-th column of J\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * point[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk; // Scaling factor D_kk\n                }\n                xNorm = Math.sqrt(xNorm); // xNorm = || D * x ||\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            // Check if J^T * r is close to zero. J^T r = P * R^T * Q^T * r = P * R^T * g\n            double maxCosine = 0;\n            if (cost != 0) { // cost = ||r|| = ||target - f(point)||\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j]; // Index in permuted R\n                    double s  = jacNorm[pj]; // Norm of j-th column of J (permuted)\n                    if (s != 0) {\n                        // Calculate (R^T * g)_j = sum_{i=0}^{j} R_{i,j} * g_i\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += jacobian[i][pj] * residuals[i]; // R_{i,j} * g_i\n                        }\n                        // Check cosine similarity: |(J^T r)_j| / (||col_j|| * ||r||)\n                        maxCosine = Math.max(maxCosine, Math.abs(sum) / (s * cost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // convergence has been reached (gradient is perpendicular to columns, i.e., J^T r is small)\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < cols; ++j) {\n                diag[j] = Math.max(diag[j], jacNorm[j]); // Update scaling D based on current Jacobian\n            }\n\n            // inner loop - search for a step p such that F(x+p) < F(x)\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = point[pj];\n                }\n                double previousCost = cost; // ||r_old|| = ||target - f(x_old)||\n                double[] tmpVec = residuals; // residuals holds g = Q^T r_old\n                residuals = oldRes; // residuals becomes scratch space\n                oldRes    = tmpVec; // oldRes holds g = Q^T r_old\n\n                // determine the Levenberg-Marquardt parameter lmPar and step p = lmDir\n                // Solves (R^T R + lmPar D^T D) p_solver = -R^T g\n                determineLMParameter(oldRes, delta, diag, work1, work2, work3);\n                // lmDir now holds p_solver\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    // Assume determineLMParameter solved for -R^T g, so lmDir is the step p.\n                    // The negation implies it might have solved for +R^T g. We keep it for now.\n                    lmDir[pj] = -lmDir[pj]; // p_actual = -p_solver\n                    point[pj] = oldX[pj] + lmDir[pj]; // x_new = x_old + p_actual\n                    double s = diag[pj] * lmDir[pj]; // (D * p_actual)_j\n                    lmNorm  += s * s;\n                }\n                lmNorm = Math.sqrt(lmNorm); // lmNorm = || D * p_actual ||\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = Math.min(delta, lmNorm);\n                }\n\n                // evaluate the function at x + p and calculate its norm\n                updateResidualsAndCost(); // Updates cost = ||r_new||, residuals = r_new = target - f(x_new)\n                                        // Also updates objective array to r_new for the checker\n                current = new VectorialPointValuePair(point, objective);\n\n                // compute the scaled actual reduction: (||r_old||^2 - ||r_new||^2) / ||r_old||^2\n                double actRed = -1.0;\n                if (0.1 * cost < previousCost) { // Ensure previousCost is not too small\n                    double r = cost / previousCost; // ||r_new|| / ||r_old||\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                // Compute work1 = R * p_actual\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj]; // Component j of p_actual (permuted)\n                    work1[j] = 0; // Initialize work1[j] -- THIS IS A BUG, should initialize before inner loop\n                                  // Corrected: inner loop correctly calculates work1[i] = sum_{j>=i} R_{ij} * dirJ\n                    // The following loop computes work1 = R * p_actual where R is upper triangular stored in jacobian\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += jacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j]; // coeff1 = || R * p_actual ||^2\n                }\n                double pc2 = previousCost * previousCost; // pc2 = ||r_old||^2\n                coeff1 = coeff1 / pc2; // Scaled || R * p_actual ||^2\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2; // Scaled lmPar * || D * p_actual ||^2\n\n                // Predicted reduction: (L(0) - L(p)) / ||r_old||^2 approx (||Jp||^2 + lmPar ||Dp||^2) / ||r_old||^2\n                // Assuming ||Jp|| = ||Rp||\n                // FIX: The original code had coeff1 + 2 * coeff2\n                double preRed = coeff1 + coeff2;\n\n                // Directional derivative: grad(C)^T p / ||r_old||^2 = -(||Jp||^2 + lmPar ||Dp||^2) / ||r_old||^2\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                // ratio = actRed / preRed = [ (||r_old||^2 - ||r_new||^2) / ||r_old||^2 ] / [ (||Rp||^2 + lmPar ||Dp||^2) / ||r_old||^2 ]\n                // ratio = (||r_old||^2 - ||r_new||^2) / (||Rp||^2 + lmPar ||Dp||^2)\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    // step was bad, reduce trust region/increase damping\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * cost >= previousCost) || (tmp < 0.1)) { // Ensure tmp is not too small\n                            tmp = 0.1;\n                        }\n                        delta = tmp * Math.min(delta, 10.0 * lmNorm); // Decrease delta\n                        lmPar /= tmp; // Increase lmPar\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    // step was good, increase trust region/decrease damping\n                    delta = 2 * lmNorm; // Increase delta (related to step size)\n                    lmPar *= 0.5; // Decrease lmPar\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm of the variables\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < cols; ++k) {\n                        double xK = diag[k] * point[k];\n                        xNorm    += xK * xK;\n                    }\n                    xNorm = Math.sqrt(xNorm); // || D * x_new ||\n\n                    // tests for convergence.\n                    // Check if checker is provided or use internal tests.\n                } else {\n                    // failed iteration, reset the previous values\n                    cost = previousCost; // Restore cost = ||r_old||\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        point[pj] = oldX[pj]; // Restore point x_old\n                    }\n                    tmpVec    = residuals; // Swap residuals/oldRes back\n                    residuals = oldRes;    // residuals holds g = Q^T r_old\n                    oldRes    = tmpVec;    // oldRes is scratch\n                    // Reset current VectorialPointValuePair to previous? No, previous is saved at start of loop.\n                    current = previous;\n                }\n\n                // Check convergence AFTER potentially restoring previous state if iteration failed\n                if (checker==null) {\n                    // Default convergence criteria\n                    if (((Math.abs(actRed) <= costRelativeTolerance) && // Relative change in cost is small\n                         (preRed <= costRelativeTolerance) && // Predicted relative change is small\n                         (ratio <= 2.0)) || // Actual change is not much larger than predicted\n                        (delta <= parRelativeTolerance * xNorm)) { // Step size (trust region) is small relative to parameters\n                        return current; // Return current (could be previous if iteration failed)\n                    }\n                 } else {\n                     if (checker.converged(getIterations(), previous, current)) {\n                         return current;\n                     }\n                 }\n\n                // check termination conditions (stagnation or too small tolerances)\n                if ((Math.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                            costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                            parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                            orthoTolerance);\n                }\n\n                // If iteration was successful, break inner loop and start next outer iteration\n                // If iteration failed, ratio < 1.0e-4, continue inner loop to try different lmPar/delta\n                if (ratio >= 1.0e-4) {\n                   break; // Go to next outer iteration\n                }\n            } // End inner loop (ratio >= 1.0e-4 breaks or max iterations exceeded implicitly?)\n             // If inner loop finishes without finding a good step (ratio stays < 1.0e-4),\n             // the outer loop continues, potentially recomputing Jacobian etc.\n             // This seems like it could loop indefinitely if determineLMParameter keeps failing.\n             // However, lmPar increases in the inner loop, eventually forcing a small step.\n\n        } // End outer loop (should be terminated by return or exception)\n\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": true,
    "test": false
}