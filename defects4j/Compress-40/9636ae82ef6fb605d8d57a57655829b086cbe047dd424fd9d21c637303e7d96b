{
    "generation": "    /**\n     * Returns at most 63 bits read from the underlying stream.\n     *\n     * @param count the number of bits to read, must be a positive\n     * number not bigger than 63.\n     * @return the bits concatenated as a long using the stream's byte order.\n     *         -1 if the end of the underlying stream has been reached before reading\n     *         the requested number of bits\n     * @throws IOException on error\n     */\n    public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) {\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n        }\n        while (bitsCachedSize < count) {\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                return nextByte;\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                // For LITTLE_ENDIAN, new bytes are added to the higher bit positions.\n                // The current bitsCachedSize is the lowest available bit index for the new byte.\n                // This accumulation strategy is less prone to losing leading bits from 'bitsCached' itself\n                // because new data is OR-ed in. However, if (nextByte << bitsCachedSize) exceeds 63 bits,\n                // Java's long truncation might occur for the MSBs of nextByte if it causes the conceptual\n                // value to exceed 64 bits. This is less critical as Little Endian prioritizes lower bits.\n                bitsCached |= (nextByte << bitsCachedSize);\n            } else { // BIG_ENDIAN\n                // For BIG_ENDIAN, new bytes are added to the lower bit positions,\n                // and existing bits are shifted left to make space.\n                // This means existing MSBs can be truncated if bitsCachedSize + 8 > 64.\n                // To prevent this, we must ensure bitsCached does not exceed 64 bits before the left shift.\n                // If bitsCachedSize is already large (e.g., 57, 58, 59, 60...), left-shifting it by 8\n                // would push its most significant bits (which are the earliest bytes read) out of the long's capacity.\n                // We need to discard these excess leading bits by right-shifting bitsCached and adjusting bitsCachedSize\n                // BEFORE the left shift. The goal is to keep the most recent 64-8=56 bits available.\n                // The number of bits to discard from the MSB of bitsCached is (bitsCachedSize - 56).\n                if (bitsCachedSize > 56) {\n                    bitsCached >>>= (bitsCachedSize - 56);\n                    bitsCachedSize = 56;\n                }\n                bitsCached <<= 8;\n                bitsCached |= nextByte;\n            }\n            bitsCachedSize += 8;\n        }\n            // bitsCachedSize >= 57 and left-shifting it 8 bits would cause an overflow\n        \n        final long bitsOut;\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsOut = (bitsCached & MASKS[count]);\n                bitsCached >>>= count;\n            } else { // BIG_ENDIAN\n                bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n                // For BIG_ENDIAN, the consumed bits must also be discarded from the cache\n                // by right-shifting. This was missing in the original code, causing\n                // bitsCached to retain stale bits and potentially accumulate beyond its useful size.\n                bitsCached >>>= count;\n            }\n            bitsCachedSize -= count;\n        return bitsOut;\n    }\n",
    "exact_match": false,
    "ast_match": false,
    "compile": true,
    "test": false
}