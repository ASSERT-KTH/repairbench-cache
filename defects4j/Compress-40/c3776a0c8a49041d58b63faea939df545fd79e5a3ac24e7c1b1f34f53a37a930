{
    "generation": "    /**\n     * Returns at most 63 bits read from the underlying stream.\n     *\n     * @param count the number of bits to read, must be a positive\n     * number not bigger than 63.\n     * @return the bits concatenated as a long using the stream's byte order.\n     *         -1 if the end of the underlying stream has been reached before reading\n     *         the requested number of bits\n     * @throws IOException on error\n     */\n    public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) {\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n        }\n        while (bitsCachedSize < count) {\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                // End of stream reached before reading enough bits.\n                // Return -1 as per Javadoc.\n                return nextByte;\n            }\n\n            // Check for potential overflow of bitsCached when adding the next byte.\n            // MAXIMUM_CACHE_SIZE is 63. If bitsCachedSize + 8 exceeds 63,\n            // a 'long' (64 bits) can still hold the value, but if count is 63,\n            // and bitsCachedSize goes to 67, operations like '<<=' for BIG_ENDIAN\n            // or '<< bitsCachedSize' for LITTLE_ENDIAN will cause bits to wrap around or be lost,\n            // as they exceed the 63rd index (highest bit) of a long.\n            // This behavior is consistent with the original project's fix for COMPRESS-363.\n            if (bitsCachedSize + 8 > MAXIMUM_CACHE_SIZE) {\n                throw new IOException(\"Attempt to read more than \" + MAXIMUM_CACHE_SIZE +\n                                      \" bits in a row, which is not supported by current BitInputStream implementation.\");\n            }\n\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                // For Little Endian, new bytes are added to the higher bit positions.\n                // bitsCached |= (nextByte << bitsCachedSize) correctly concatenates.\n                bitsCached |= (nextByte << bitsCachedSize);\n            } else { // BIG_ENDIAN\n                // For Big Endian, existing bits are shifted left to make space for the new byte\n                // at the lowest positions, effectively putting the new byte to the right of existing bits.\n                bitsCached <<= 8;\n                bitsCached |= nextByte;\n            }\n            bitsCachedSize += 8;\n        }\n        \n        final long bitsOut;\n        if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n            // For Little Endian, the lowest 'count' bits of bitsCached are the desired bits.\n            bitsOut = (bitsCached & MASKS[count]);\n            // Remove the consumed bits from bitsCached by right-shifting.\n            bitsCached >>>= count;\n        } else { // BIG_ENDIAN\n            // For Big Endian, the highest 'count' bits of bitsCached are the desired bits.\n            // They are extracted by shifting right and then masking.\n            bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n            // BUG FIX: The 'bitsCached' was not updated to remove the 'count' bits that were just read.\n            // This left old data in 'bitsCached', affecting subsequent reads.\n            // After extracting the highest 'count' bits, the remaining 'bitsCachedSize - count'\n            // least significant bits should be preserved. This is achieved by masking.\n            bitsCached &= MASKS[bitsCachedSize - count];\n        }\n        bitsCachedSize -= count;\n        return bitsOut;\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": true,
    "test": false
}