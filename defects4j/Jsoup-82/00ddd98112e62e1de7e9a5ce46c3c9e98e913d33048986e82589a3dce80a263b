{
    "generation": "    static Document parseInputStream(InputStream input, String charsetName, String baseUri, Parser parser) throws IOException  {\n        if (input == null) // empty body\n            return new Document(baseUri);\n        input = ConstrainableInputStream.wrap(input, bufferSize, 0);\n\n        Document doc = null;\n        boolean fullyRead = false;\n\n        // read the start of the stream and look for a BOM or meta charset\n        input.mark(bufferSize);\n        ByteBuffer firstBytes = readToByteBuffer(input, firstReadBufferSize - 1); // -1 because we read one more to see if completed. First read is < buffer size, so can't be invalid.\n        fullyRead = input.read() == -1;\n        input.reset();\n\n        // look for BOM - overrides any other header or input\n        BomCharset bomCharset = detectCharsetFromBom(firstBytes);\n        if (bomCharset != null)\n            charsetName = bomCharset.charset;\n\n        if (charsetName == null) { // determine from meta. safe first parse as UTF-8\n            // Make slice of the firstBytes, rather than a view, so if buffer is larger than doc, rest of bytes aren't loaded.\n            ByteBuffer PEEK_BYTES = firstBytes.slice(); // backup, in case need to re-read with detected charset\n            String docData = Charset.forName(defaultCharset).decode(PEEK_BYTES).toString();\n            doc = parser.parseInput(docData, baseUri);\n\n            // look for <meta http-equiv=\"Content-Type\" content=\"text/html;charset=gb2312\"> or HTML5 <meta charset=\"gb2312\">\n            Elements metaElements = doc.select(\"meta[http-equiv=content-type], meta[charset]\");\n            String foundCharset = null; // if not found, will keep utf-8 as best attempt\n            for (Element meta : metaElements) {\n                if (meta.hasAttr(\"http-equiv\"))\n                    foundCharset = getCharsetFromContentType(meta.attr(\"content\"));\n                if (foundCharset == null && meta.hasAttr(\"charset\"))\n                    foundCharset = meta.attr(\"charset\");\n                if (foundCharset != null)\n                    break;\n            }\n\n            // look for <?xml encoding='ISO-8859-1'?>\n            if (foundCharset == null && doc.childNodeSize() > 0) {\n                Node first = doc.childNode(0);\n                XmlDeclaration decl = null;\n                if (first instanceof XmlDeclaration)\n                    decl = (XmlDeclaration) first;\n                else if (first instanceof Comment) {\n                    Comment comment = (Comment) first;\n                    if (comment.isXmlDeclaration())\n                        decl = comment.asXmlDeclaration();\n                }\n                if (decl != null) {\n                    if (decl.name().equalsIgnoreCase(\"xml\"))\n                        foundCharset = decl.attr(\"encoding\");\n                }\n            }\n            foundCharset = validateCharset(foundCharset);\n            if (foundCharset != null && !foundCharset.equalsIgnoreCase(defaultCharset)) { // need to re-decode. (case insensitive check here to match how validate works)\n                foundCharset = foundCharset.trim().replaceAll(\"[\\\"']\", \"\");\n                charsetName = foundCharset;\n                doc = null; // discard doc and parse again\n            } else if (!fullyRead) { // if UTF-8 wasn't explicit via meta tag, and we haven't read the whole stream, maybe give up and re-parse with final encoding\n                doc = null;\n            }\n        } else { // specified by content type header (or by user on file load)\n            Validate.notEmpty(charsetName, \"Must set charset arg to character set of file to parse. Set to null to attempt to detect from HTML\");\n        }\n        if (doc == null) { // If doc is null, we need to parse it from the input stream, possibly with a detected charset.\n            if (charsetName == null)\n                charsetName = defaultCharset; // Determines the character encoding to use for the InputStreamReader\n\n            BufferedReader reader = new BufferedReader(new InputStreamReader(input, charsetName), bufferSize);\n            if (bomCharset != null && bomCharset.offset) { // creating the buffered reader ignores the input pos, so must skip here\n                long skipped = reader.skip(1); // Bom is always 1 character, regardless of actual encoding byte count.\n                Validate.isTrue(skipped == 1);\n            }\n            try {\n                doc = parser.parseInput(reader, baseUri);\n            } catch (UncheckedIOException e) {\n                // io exception when parsing (not seen before because reading the stream as we go)\n                throw e.ioException();\n            }\n\n            // We determine the charset name from the stream byte sequences,\n            // and then set the output stream appropriately.\n            Charset charset = Charset.forName(charsetName);\n            doc.outputSettings().charset(charset); // Initially set the output charset based on determined/provided name\n\n            // Check if the determined charset can actually be encoded. If not, swap to default (UTF-8)\n            // and update the document's meta element. This is useful for reading docs from CJK\n            // charsets that may not be supported on the writing JVM.\n            if (!charset.canEncode()) {\n                doc.charset(Charset.forName(defaultCharset)); // This updates the output charset AND the meta tag.\n            }\n        }\n        input.close(); // Ensure the input stream is closed, even if fully read before.\n        return doc;\n    }",
    "exact_match": false,
    "ast_match": false,
    "compile": true,
    "test": false
}